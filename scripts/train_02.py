import argparse
import logging
import os
import shutil
import sys
import time
import types
import typing
from dataclasses import dataclass, fields
from pathlib import Path

# Rich library for beautiful console output
import rich
import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn as nn
import torch.optim as optim
from rich.logging import RichHandler
from rich.progress import BarColumn, Progress, TextColumn, TimeElapsedColumn, TimeRemainingColumn
from rich.table import Table
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler, TensorDataset


# ============================================================================
# 1. æ‰å¹³åŒ–é…ç½® (Simpler Configuration)
# ============================================================================
@dataclass
class TrainingConfig:
    """å°†æ‰€æœ‰é…ç½®åˆå¹¶åˆ°ä¸€ä¸ªç±»ä¸­ï¼Œæ›´æ˜“äºŽç®¡ç†å’Œä¼ é€’"""

    # Model
    hidden_size: int = 512
    ffn_hidden_size: int = 2048
    num_layers: int = 2
    dropout: float = 0.1
    # Training
    batch_size: int = 128
    epochs: int = 10
    lr: float = 1e-3
    weight_decay: float = 0.01
    gradient_clip_val: float = 1.0
    # Data
    num_samples: int = 20000
    # DDP
    backend: str = "nccl"
    # Optimization
    use_compile: bool = True
    use_amp: bool = True
    num_workers: int = 4
    pin_memory: bool = True
    # Checkpointing & Logging
    checkpoint_dir: str = "checkpoints"
    resume_from: str | None = None
    log_dir: str = "logs"


def get_config() -> TrainingConfig:
    """é€šè¿‡ argparse è§£æžå‘½ä»¤è¡Œå‚æ•°æ¥è¦†ç›–é»˜è®¤é…ç½®"""
    parser = argparse.ArgumentParser(description="A streamlined PyTorch DDP training script with rich logging.")
    config = TrainingConfig()

    for f in fields(config):
        type_for_argparse = f.type
        origin = typing.get_origin(f.type)
        if origin is types.UnionType or origin is typing.Union:
            base_type = next((t for t in typing.get_args(f.type) if t is not types.NoneType), None)
            if base_type:
                type_for_argparse = base_type

        if type_for_argparse is bool:
            if f.default:
                parser.add_argument(
                    f"--no-{f.name.replace('_', '-')}", action="store_false", dest=f.name, help=f"Disable {f.name}"
                )
            else:
                parser.add_argument(
                    f"--{f.name.replace('_', '-')}", action="store_true", dest=f.name, help=f"Enable {f.name}"
                )
        else:
            parser.add_argument(
                f"--{f.name.replace('_', '-')}", type=type_for_argparse, default=None, help=f"Default: {f.default}"
            )

    args = parser.parse_args()

    for f in fields(config):
        arg_val = getattr(args, f.name)
        if arg_val is not None:
            setattr(config, f.name, arg_val)

    return config


# ============================================================================
# 2. è¾…åŠ©å‡½æ•° (Utility Functions)
# ============================================================================
def setup_logging(rank: int):
    """è®¾ç½®æ—¥å¿—ï¼Œä¸»è¿›ç¨‹ä½¿ç”¨ RichHandlerï¼Œå…¶ä»–è¿›ç¨‹é™é»˜"""
    logger = logging.getLogger(f"train_rank_{rank}")
    logger.propagate = False

    if rank == 0:
        logger.setLevel(logging.INFO)
        handler = RichHandler(rich_tracebacks=True, show_path=False, markup=True)
        handler.setFormatter(logging.Formatter("%(message)s", datefmt="[%X]"))
        logger.addHandler(handler)
    else:
        logger.setLevel(logging.WARNING)
        logger.addHandler(logging.NullHandler())
    return logger


def setup_ddp(rank: int, world_size: int, backend: str):
    """è®¾ç½® DDP çŽ¯å¢ƒ"""
    os.environ["MASTER_ADDR"] = os.getenv("MASTER_ADDR", "localhost")
    os.environ["MASTER_PORT"] = os.getenv("MASTER_PORT", "12355")
    dist.init_process_group(backend, rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)
    torch.manual_seed(42 + rank)
    torch.cuda.manual_seed_all(42 + rank)


def cleanup_ddp():
    if dist.is_initialized():
        dist.destroy_process_group()


# ============================================================================
# 3. æ¨¡åž‹ (Model Definition)
# ============================================================================
class SimpleMLP(nn.Module):
    def __init__(self, config: TrainingConfig):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(config.hidden_size, config.ffn_hidden_size),
            nn.GELU(),
            nn.Dropout(config.dropout),
            nn.Linear(config.ffn_hidden_size, config.hidden_size),
        )
        self._init_weights()

    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)


# ============================================================================
# 4. è®­ç»ƒå™¨ (The Streamlined Trainer with Rich Integration)
# ============================================================================
class Trainer:
    def __init__(self, config: TrainingConfig, rank: int, world_size: int, logger: logging.Logger):
        self.config = config
        self.rank = rank
        self.world_size = world_size
        self.logger = logger
        self.device = torch.device(f"cuda:{rank}")

        model = SimpleMLP(self.config).to(self.device)
        if self.config.use_compile and sys.version_info >= (3, 8):
            self.logger.info("Compiling model with torch.compile...")
            model = torch.compile(model)
        self.model = DDP(model, device_ids=[self.rank])

        self.train_loader, self.sampler = self._create_dataloader()

        self.optimizer = optim.AdamW(
            self.model.parameters(), lr=self.config.lr, weight_decay=self.config.weight_decay, fused=True
        )
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.config.epochs)
        self.scaler = torch.amp.GradScaler(enabled=self.config.use_amp)
        self.criterion = nn.MSELoss()

        self.start_epoch = 0
        self.best_loss = float("inf")
        if self.config.resume_from:
            self._load_checkpoint(self.config.resume_from)

    def _create_dataloader(self) -> tuple[DataLoader, DistributedSampler]:
        dataset = TensorDataset(
            torch.randn(self.config.num_samples, self.config.hidden_size),
            torch.randn(self.config.num_samples, self.config.hidden_size),
        )
        sampler = DistributedSampler(
            dataset, num_replicas=self.world_size, rank=self.rank, shuffle=True, drop_last=True
        )
        loader = DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            sampler=sampler,
            num_workers=self.config.num_workers,
            pin_memory=self.config.pin_memory,
            persistent_workers=self.config.num_workers > 0,
        )
        return loader, sampler

    def _save_checkpoint(self, epoch: int, loss: float):
        if self.rank != 0:
            return
        checkpoint_dir = Path(self.config.checkpoint_dir)
        checkpoint_dir.mkdir(parents=True, exist_ok=True)

        state = {
            "epoch": epoch,
            "loss": loss,
            "best_loss": self.best_loss,
            "model_state": self.model.module.state_dict(),
            "optimizer_state": self.optimizer.state_dict(),
            "scheduler_state": self.scheduler.state_dict(),
            "scaler_state": self.scaler.state_dict(),
        }

        latest_path = checkpoint_dir / "latest.pt"
        temp_path = checkpoint_dir / "latest.pt.tmp"
        torch.save(state, temp_path)
        shutil.move(temp_path, latest_path)

        if loss < self.best_loss:
            self.best_loss = loss
            self.logger.info(f"ðŸ† New best model found with loss [bold yellow]{loss:.4f}[/], saving to best.pt")
            best_path = checkpoint_dir / "best.pt"
            shutil.copyfile(latest_path, best_path)

    def _load_checkpoint(self, path: str):
        if not os.path.exists(path):
            self.logger.warning(f"Checkpoint {path} not found. Starting from scratch.")
            return

        checkpoint = torch.load(path, map_location=self.device)
        self.model.module.load_state_dict(checkpoint["model_state"])
        self.optimizer.load_state_dict(checkpoint["optimizer_state"])
        self.scheduler.load_state_dict(checkpoint["scheduler_state"])
        self.scaler.load_state_dict(checkpoint["scaler_state"])
        self.start_epoch = checkpoint["epoch"] + 1
        self.best_loss = checkpoint.get("best_loss", float("inf"))
        self.logger.info(f"âœ… Resumed from checkpoint [cyan]{path}[/] at epoch {self.start_epoch}.")

    def train(self):
        if self.rank == 0:
            self.logger.info(f"ðŸŽ‰ Starting training for {self.config.epochs} epochs...")

        progress_columns = [
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TimeRemainingColumn(),
            TimeElapsedColumn(),
            TextColumn("[bold blue]Loss: {task.fields[loss]:.4f} | LR: {task.fields[lr]:.2e}"),
        ]

        total_start_time = time.time()
        with Progress(*progress_columns, transient=False) if self.rank == 0 else open(os.devnull, "w") as progress:
            epoch_task = (
                progress.add_task("[bold red]Epochs", total=self.config.epochs, loss=0.0, lr=0.0)
                if self.rank == 0
                else None
            )

            for epoch in range(self.start_epoch, self.config.epochs):
                self.sampler.set_epoch(epoch)
                self.model.train()

                epoch_loss = 0.0
                if self.rank == 0:
                    torch.cuda.reset_peak_memory_stats(self.device)

                batch_task = (
                    progress.add_task(f"[cyan]Epoch {epoch + 1}", total=len(self.train_loader), loss=0.0, lr=0.0)
                    if self.rank == 0
                    else None
                )

                for data, target in self.train_loader:
                    data, target = data.to(self.device, non_blocking=True), target.to(self.device, non_blocking=True)

                    with torch.amp.autocast(device_type="cuda", dtype=torch.bfloat16, enabled=self.config.use_amp):
                        output = self.model(data)
                        loss = self.criterion(output, target)

                    self.optimizer.zero_grad(set_to_none=True)
                    self.scaler.scale(loss).backward()
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_val)
                    self.scaler.step(self.optimizer)
                    self.scaler.update()

                    epoch_loss += loss.item()

                    if self.rank == 0:
                        progress.update(
                            batch_task, advance=1, loss=loss.item(), lr=self.optimizer.param_groups[0]["lr"]
                        )

                self.scheduler.step()

                avg_loss_tensor = torch.tensor(epoch_loss / len(self.train_loader), device=self.device)
                dist.all_reduce(avg_loss_tensor, op=dist.ReduceOp.AVG)
                avg_loss = avg_loss_tensor.item()

                if self.rank == 0:
                    peak_mem = torch.cuda.max_memory_allocated(self.device) / 1024**3
                    self.logger.info(
                        f"Epoch {epoch + 1} Summary | "
                        f"Avg Loss: [bold yellow]{avg_loss:.4f}[/] | "
                        f"Peak Mem: [bold cyan]{peak_mem:.2f} GB[/]"
                    )
                    self._save_checkpoint(epoch, avg_loss)
                    progress.update(epoch_task, advance=1)
                    progress.remove_task(batch_task)

        if self.rank == 0:
            total_time = time.time() - total_start_time
            self.logger.info(f"âœ… Training finished in [bold green]{total_time / 3600:.2f} hours[/].")


# ============================================================================
# 5. ä¸»å…¥å£ (Main Entrypoint)
# ============================================================================
def worker_fn(rank: int, world_size: int, config: TrainingConfig):
    """æ¯ä¸ª DDP è¿›ç¨‹æ‰§è¡Œçš„å‡½æ•°"""
    logger = setup_logging(rank)
    try:
        setup_ddp(rank, world_size, config.backend)
        trainer = Trainer(config, rank, world_size, logger)
        trainer.train()
    except Exception:
        logger.error(f"FATAL ERROR in rank {rank}", exc_info=True)
    finally:
        cleanup_ddp()


def main():
    config = get_config()

    torch.backends.cudnn.benchmark = True
    torch.set_float32_matmul_precision("high")

    world_size = torch.cuda.device_count()
    if world_size == 0:
        rich.print("[bold red]âŒ No GPUs found. Exiting.[/]")
        sys.exit(1)

    # åœ¨ä¸»è¿›ç¨‹ä¸­æ‰“å°ä¸€æ¬¡é…ç½®ä¿¡æ¯
    if int(os.environ.get("RANK", 0)) == 0:
        table = Table(title="ðŸ”§ Training Configuration", show_header=True, header_style="bold magenta")
        table.add_column("Parameter", style="dim", width=20)
        table.add_column("Value", style="bold")
        for f in fields(config):
            table.add_row(f.name, str(getattr(config, f.name)))
        rich.print(table)

    if world_size > 1:
        rich.print(f"ðŸš€ [bold green]Spawning {world_size} DDP processes...[/]")
        mp.spawn(worker_fn, args=(world_size, config), nprocs=world_size, join=True)
    else:
        rich.print("ðŸš€ [bold green]Starting single-GPU training...[/]")
        worker_fn(0, 1, config)


if __name__ == "__main__":
    main()
