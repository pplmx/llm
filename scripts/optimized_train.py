import argparse
import os
import time
from dataclasses import dataclass

import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn as nn
import torch.optim as optim
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler, TensorDataset


# [BEST PRACTICE] ä½¿ç”¨ dataclass ç®¡ç†é…ç½®ï¼Œæ›´æ¸…æ™°ä¸”æ”¯æŒç±»å‹æç¤º
@dataclass
class TrainConfig:
    """è®­ç»ƒé…ç½®"""

    # æ¨¡å‹å‚æ•°
    hidden_size: int = 512
    ffn_hidden_size: int = 2048

    # è®­ç»ƒå‚æ•°
    batch_size: int = 128
    epochs: int = 10
    lr: float = 1e-3
    weight_decay: float = 0.01

    # æ•°æ®é›†å‚æ•°
    num_samples: int = 20000

    # DDP ç¯å¢ƒå‚æ•°
    master_addr: str = "127.0.0.1"
    master_port: str = "12355"
    num_nodes: int = 1
    gpus_per_node: int = torch.cuda.device_count()
    node_rank: int = 0

    # æ€§èƒ½ä¼˜åŒ–å¼€å…³
    use_compile: bool = True  # æ˜¯å¦ä½¿ç”¨ torch.compile
    use_amp: bool = True  # æ˜¯å¦ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦


class SimpleMLP(nn.Module):
    def __init__(self, hidden_size=512, ffn_hidden_size=2048):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(hidden_size, ffn_hidden_size),
            nn.ReLU(),
            nn.Linear(ffn_hidden_size, hidden_size),
        )

    def forward(self, x):
        return self.net(x)


def setup_ddp(rank: int, world_size: int, config: TrainConfig):
    """
    åˆå§‹åŒ– DDP ç¯å¢ƒ
    """
    os.environ["MASTER_ADDR"] = config.master_addr
    os.environ["MASTER_PORT"] = str(config.master_port)
    dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)
    torch.cuda.set_device(rank % torch.cuda.device_count())


def cleanup_ddp():
    """
    æ¸…ç† DDP ç¯å¢ƒ
    """
    if dist.is_initialized():
        dist.destroy_process_group()


class Trainer:
    """
    å°†è®­ç»ƒé€»è¾‘å°è£…æˆä¸€ä¸ªç±»ï¼Œæé«˜ä»£ç çš„æ¨¡å—åŒ–å’Œå¯å¤ç”¨æ€§
    """

    def __init__(self, config: TrainConfig, rank: int, world_size: int):
        self.config = config
        self.rank = rank
        self.world_size = world_size
        self.device = torch.device(f"cuda:{rank % torch.cuda.device_count()}")

        # 1. åˆ›å»ºæ¨¡å‹
        model = SimpleMLP(config.hidden_size, config.ffn_hidden_size)

        # [PERFORMANCE] ä½¿ç”¨ torch.compile ä¼˜åŒ–æ¨¡å‹ (PyTorch 2.0+)
        if config.use_compile:
            if self.rank == 0:
                print("ğŸš€ Compiling the model with torch.compile()...")
            # `reduce-overhead` æ¨¡å¼å¯ä»¥å‡å°‘ç¼–è¯‘å¼€é”€ï¼Œé€‚åˆåŠ¨æ€æ€§è¾ƒå°çš„æ¨¡å‹
            model = torch.compile(model, mode="reduce-overhead")

        self.model = DDP(model.to(self.device), device_ids=[self.device.index])

        # 2. åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        # [PERFORMANCE] å¯¹æ”¯æŒçš„ä¼˜åŒ–å™¨ä½¿ç”¨ fused=True ä»¥æé«˜GPUæ€§èƒ½
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=config.lr,
            weight_decay=config.weight_decay,
            fused=torch.cuda.is_available(),  # fused ä»…åœ¨ CUDA ä¸Šå¯ç”¨
        )
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=config.epochs)

        # 3. [PERFORMANCE] åˆ›å»ºæ··åˆç²¾åº¦è®­ç»ƒçš„ GradScaler
        self.scaler = torch.amp.GradScaler("cuda", enabled=config.use_amp)

        # 4. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        self.dataloader, self.sampler = self._create_dataloader()

        self.criterion = nn.MSELoss()

    def _create_dataloader(self):
        # åœ¨æ¯ä¸ª rank ä¸Šåˆ›å»ºç›¸åŒçš„æ•°æ®ï¼ŒDDP Sampler ä¼šè´Ÿè´£åˆ†å‘
        x = torch.randn(self.config.num_samples, self.config.hidden_size)
        y = torch.randn(self.config.num_samples, self.config.hidden_size)
        dataset = TensorDataset(x, y)

        sampler = DistributedSampler(dataset, num_replicas=self.world_size, rank=self.rank, shuffle=True)
        # æ¨è num_workers > 0 ä»¥åˆ©ç”¨å¤šè¿›ç¨‹åŠ è½½æ•°æ®
        dataloader = DataLoader(
            dataset, batch_size=self.config.batch_size, sampler=sampler, num_workers=4, pin_memory=True
        )
        return dataloader, sampler

    def _run_epoch(self, epoch: int):
        self.sampler.set_epoch(epoch)  # ä¿è¯æ¯ä¸ª epoch çš„ shuffle éƒ½ä¸åŒ
        self.model.train()

        total_loss = 0.0
        for data, target in self.dataloader:
            data = data.to(self.device, non_blocking=True)
            target = target.to(self.device, non_blocking=True)

            # [PERFORMANCE] ä½¿ç”¨ set_to_none=True ç•¥å¾®æå‡æ€§èƒ½
            self.optimizer.zero_grad(set_to_none=True)

            # [PERFORMANCE] ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ (AMP)
            with torch.amp.autocast("cuda", enabled=self.config.use_amp):
                output = self.model(data)
                loss = self.criterion(output, target)

            # ä½¿ç”¨ GradScaler ç¼©æ”¾æŸå¤±å¹¶åå‘ä¼ æ’­
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()

            total_loss += loss.item()

        return total_loss / len(self.dataloader)

    def train(self):
        if self.rank == 0:
            print("ğŸ‰ Starting training...")

        start_time = time.time()
        for epoch in range(self.config.epochs):
            avg_loss = self._run_epoch(epoch)
            self.scheduler.step()

            # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆå½“å‰ epoch
            dist.barrier()

            if self.rank == 0:
                elapsed = time.time() - start_time
                lr = self.scheduler.get_last_lr()[0]
                print(
                    f"ğŸ“Š Epoch {epoch + 1:2d}/{self.config.epochs} | "
                    f"Loss: {avg_loss:.4f} | LR: {lr:.6f} | Time: {elapsed:.1f}s"
                )

        if self.rank == 0:
            total_time = time.time() - start_time
            print(f"âœ… Training completed in {total_time:.1f}s on {self.world_size} GPUs.")


def train_worker(rank: int, world_size: int, config: TrainConfig):
    """
    ä¸ºæ¯ä¸ª DDP è¿›ç¨‹æ‰§è¡Œçš„ worker å‡½æ•°
    """
    try:
        setup_ddp(rank, world_size, config)
        trainer = Trainer(config, rank, world_size)
        trainer.train()
    except Exception as e:
        print(f"Error in rank {rank}: {e}")
        # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯å¤„ç†
    finally:
        cleanup_ddp()


def get_config_from_env_and_args():
    """
    [BEST PRACTICE] ç»“åˆ argparse å’Œç¯å¢ƒå˜é‡æ¥é…ç½®ï¼Œæ›´åŠ çµæ´»
    """
    parser = argparse.ArgumentParser(description="PyTorch DDP Training Script")
    # å…è®¸å‘½ä»¤è¡Œè¦†ç›–é»˜è®¤é…ç½®
    parser.add_argument("--epochs", type=int, help="Number of training epochs.")
    parser.add_argument("--batch_size", type=int, help="Batch size per GPU.")
    parser.add_argument("--lr", type=float, help="Learning rate.")
    parser.add_argument("--no-compile", action="store_true", help="Disable torch.compile.")

    args = parser.parse_args()

    # ä»ç¯å¢ƒå˜é‡è·å–åˆ†å¸ƒå¼è®¾ç½®ï¼Œè¿™æ˜¯åœ¨é›†ç¾¤ç¯å¢ƒä¸­ï¼ˆå¦‚ SLURMï¼‰çš„æ ‡å‡†åšæ³•
    config_dict = {
        "node_rank": int(os.environ.get("NODE_RANK", 0)),
        "num_nodes": int(os.environ.get("NUM_NODES", 1)),
        "master_addr": os.environ.get("MASTER_ADDR", "127.0.0.1"),
        "master_port": os.environ.get("MASTER_PORT", "12355"),
        "gpus_per_node": int(os.environ.get("GPUS_PER_NODE", torch.cuda.device_count())),
    }

    # å‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆäºé»˜è®¤å€¼
    if args.epochs is not None:
        config_dict["epochs"] = args.epochs
    if args.batch_size is not None:
        config_dict["batch_size"] = args.batch_size
    if args.lr is not None:
        config_dict["lr"] = args.lr
    if args.no_compile:
        config_dict["use_compile"] = False

    return TrainConfig(**config_dict)


def main():
    """ä¸»å‡½æ•°ï¼Œå¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ"""
    config = get_config_from_env_and_args()

    # [PERFORMANCE] å¯ç”¨ CuDNN benchmark æ¨¡å¼
    torch.backends.cudnn.benchmark = True

    world_size = config.num_nodes * config.gpus_per_node

    if world_size > 1:
        print("ğŸ”§ Distributed Training Configuration:")
        print(f"   ğŸŒ World Size: {world_size} | Nodes: {config.num_nodes} | GPUs per node: {config.gpus_per_node}")
        print(f"   ğŸ·ï¸   Current node rank: {config.node_rank}")
        print(f"   ğŸŒ Master: {config.master_addr}:{config.master_port}")
        print(f"   âš™ï¸  torch.compile: {'Enabled' if config.use_compile else 'Disabled'}")
        print(f"   âš¡ AMP: {'Enabled' if config.use_amp else 'Disabled'}")
        print("=" * 60)

        mp.spawn(train_worker, args=(world_size, config), nprocs=config.gpus_per_node, join=True)
    elif world_size == 1:
        print("ğŸš€ Starting single-GPU training...")
        train_worker(0, 1, config)
    else:
        print("No GPUs found. Exiting.")


if __name__ == "__main__":
    main()
