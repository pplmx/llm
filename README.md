# llm

[![Generated by](https://img.shields.io/badge/Generated%20by-x--pt%2Ftemplate-blue)](https://github.com/x-pt/template)
[![CI](https://github.com/pplmx/llm/workflows/CI/badge.svg)](https://github.com/pplmx/llm/actions)
[![Coverage Status](https://coveralls.io/repos/github/pplmx/llm/badge.svg?branch=main)](https://coveralls.io/github/pplmx/llm?branch=main)
[![PyPI version](https://badge.fury.io/py/llm.svg)](https://badge.fury.io/py/llm)
[![Python Versions](https://img.shields.io/pypi/pyversions/llm.svg)](https://pypi.org/project/llm/)

## Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Quick Start](#quick-start)
- [Installation](#installation)
- [Usage](#usage)
- [Development](#development)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [License](#license)
- [Changelog](#changelog)
- [Contact](#contact)
- [Acknowledgements](#acknowledgements)

## Overview

`llm` is a modular and extensible PyTorch framework for training and experimenting with Large Language Models (LLMs). It provides a robust infrastructure for building, training, and evaluating LLMs, focusing on modularity, scalability, and ease of experimentation.

## Features

- **Modular LLM Architecture**:
    - Built with a modular Transformer architecture, allowing for flexible component swapping.
    - Supports advanced features like Mixture of Experts (MoE) and various normalization techniques (Pre-LN/Post-LN).
- **Robust Training Framework**:
    - Distributed Data Parallel (DDP) training for scalability.
    - Automatic Mixed Precision (AMP) for memory efficiency and faster training.
    - `torch.compile` integration for performance optimization.
    - Flexible configuration system using YAML and Python dataclasses.
    - Extensible callback system for custom training logic (logging, checkpointing, early stopping, LR scheduling).
    - Comprehensive checkpoint management and performance monitoring.
- **Efficient Development Workflow**:
    - Utilizes `uv` for fast and reliable dependency management.
    - Enforces code quality with `ruff` (linting and formatting) and `mypy` (static type checking).
    - Comprehensive testing with `pytest`, including coverage reports and Allure test results.
- **Data and Tokenization Abstraction**:
    - Modular `DataModule` design for flexible data loading and preprocessing.
    - Character-level tokenizer for basic experimentation, with clear extensibility for advanced tokenizers.
- **Example Training Script**: The `src/llm/training/train.py` script demonstrates end-to-end training of decoder-only models, showcasing the framework's capabilities.

## Quick Start

To quickly get started with training a model using the framework, follow these steps:

1.  **Initialize Project**: Ensure your environment is set up by running the initialization command (see [Installation](#installation) for details).
    ```bash
    make init
    ```
2.  **Run a Training Example**: Execute the main training script with a sample task.
    ```bash
    python src/llm/training/train.py --task regression --epochs 1 --batch-size 32
    ```
    This command will train a simple regression model for 1 epoch with a batch size of 32. You will see training progress logs in your console.

## Installation

### Requirements
- Python 3.13+
- [uv](https://github.com/astral-sh/uv): A fast Python package installer and resolver, written in Rust.
- [make](https://www.gnu.org/software/make/): A build automation tool (typically pre-installed on Linux/macOS, available via Chocolatey/Scoop on Windows).

### Setting up the Environment

This project uses `uv` for dependency management and `Makefile` for common development tasks.

1.  **Install `uv`**: If you don't have `uv` installed, follow the official instructions [here](https://github.com/astral-sh/uv#installation).
2.  **Initialize Project**: Navigate to the project root directory and run the `make init` command. This will set up the virtual environment, install all necessary dependencies, and install pre-commit hooks.
    ```bash
    make init
    ```
3.  **Synchronize Dependencies (if needed)**: If `pyproject.toml` or `uv.lock` changes, you can re-synchronize dependencies:
    ```bash
    make sync
    ```

### User Installation (Distribution)
If this project were to be distributed as a package, users would typically install it using pip:

```bash
pip install llm # Assuming 'llm' is the package name on PyPI
```
However, for development, `make init` is the recommended way to set up the environment.

## Usage

For comprehensive documentation, including detailed usage examples, development guides, and troubleshooting, please refer to our dedicated documentation section:

-   [Development Guide](docs/development.md)
-   [CPU LLM Tutorial](docs/tutorial-cpu-llm.md)
-   [Project Troubleshooting](docs/troubleshooting.md)
-   [Training Framework Documentation](docs/training/README.md)


## Development

For detailed information on setting up the development environment, running tests, maintaining code quality, and other development workflows, please refer to our comprehensive [Development Guide](docs/development.md).

## Troubleshooting

If you encounter any issues while using `llm`, please check our [Troubleshooting Guide](docs/troubleshooting.md) for common problems and their solutions. If you can't find a solution to your problem, please [open an issue](https://github.com/pplmx/llm/issues) on our GitHub repository.

## Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details on how to submit pull requests, report issues, or suggest improvements.

## License

This project is licensed under either of:

- Apache License, Version 2.0 ([LICENSE-APACHE](LICENSE-APACHE) or http://www.apache.org/licenses/LICENSE-2.0)
- MIT license ([LICENSE-MIT](LICENSE-MIT) or http://opensource.org/licenses/MIT)

at your option.

## Changelog

For a detailed history of changes to this project, please refer to our [CHANGELOG.md](CHANGELOG.md).

## Contact

For questions, suggestions, or support, please open an issue on our GitHub repository.

## Acknowledgements

We acknowledge all contributors, open-source projects, and resources that have inspired and supported the development of this project.
