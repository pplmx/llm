# æŒ‡å—ï¼šæ‰©å±•è®­ç»ƒæ¡†æ¶ (`GUIDE_EXTENDING.md`)

æœ¬æ¡†æ¶çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºå…¶å¯æ‰©å±•æ€§ã€‚æœ¬æŒ‡å—å°†é€šè¿‡å‡ ä¸ªâ€œé£Ÿè°±å¼â€çš„ç¤ºä¾‹ï¼Œå‘æ‚¨å±•ç¤ºå¦‚ä½•è½»æ¾åœ°æ·»åŠ æ–°åŠŸèƒ½ã€‚

---

### é£Ÿè°±1ï¼šå¦‚ä½•æ·»åŠ ä¸€ä¸ªæ–°çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Ÿ

å‡è®¾æˆ‘ä»¬æƒ³æ·»åŠ ä¸€ä¸ª `ExponentialLR` è°ƒåº¦å™¨ã€‚

**ç¬¬1æ­¥ï¼šåœ¨ `config.py` ä¸­æ·»åŠ é€‰é¡¹**

åœ¨ `TrainingConfig` æ•°æ®ç±»ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä¸º `scheduler_type` æ·»åŠ ä¸€ä¸ªæ–°çš„æœ‰æ•ˆé€‰é¡¹çš„æ³¨é‡Šï¼Œä»¥æ–¹ä¾¿å…¶ä»–å¼€å‘è€…çŸ¥é“å®ƒçš„å­˜åœ¨ã€‚

```python
# in core/config.py
@dataclass
class TrainingConfig:
    # ...
    scheduler_type: str = "cosine"  # æ–°å¢: cosine, step, plateau, exponential
    # ...
```

**ç¬¬2æ­¥ï¼šåœ¨ `TrainingTask` ä¸­å®ç°é€»è¾‘**

åœ¨æ‚¨çš„ `TrainingTask` å­ç±»ï¼ˆä¾‹å¦‚ `RegressionTask`ï¼‰çš„ `build_scheduler` æ–¹æ³•ä¸­ï¼Œæ·»åŠ å¯¹æ–°è°ƒåº¦å™¨çš„æ”¯æŒã€‚

```python
# in tasks/regression_task.py
from torch.optim.lr_scheduler import ExponentialLR # å¯¼å…¥æ–°çš„è°ƒåº¦å™¨

class RegressionTask(TrainingTask):
    # ...
    def build_scheduler(self, optimizer: optim.Optimizer) -> LRScheduler | None:
        scheduler_map = {
            "cosine": optim.lr_scheduler.CosineAnnealingLR(...),
            "step": optim.lr_scheduler.StepLR(...),
            "plateau": optim.lr_scheduler.ReduceLROnPlateau(...), # å‡è®¾å·²å­˜åœ¨
            "exponential": ExponentialLR(optimizer, gamma=0.9), # <-- æ–°å¢é€»è¾‘
        }
        scheduler = scheduler_map.get(self.config.training.scheduler_type)
        # ... åç»­çš„ warmup é€»è¾‘ä¿æŒä¸å˜ ...
        return scheduler
```

ç°åœ¨ï¼Œæ‚¨å°±å¯ä»¥é€šè¿‡é…ç½® `--scheduler-type exponential` æ¥ä½¿ç”¨æ–°çš„è°ƒåº¦å™¨äº†ã€‚

---

### é£Ÿè°±2ï¼šå¦‚ä½•æ·»åŠ ä¸€ä¸ªæ–°çš„å›è°ƒï¼Ÿ

å‡è®¾æˆ‘ä»¬æƒ³åˆ›å»ºä¸€ä¸ªåœ¨è®­ç»ƒå¼€å§‹å’Œç»“æŸæ—¶æ‰“å°ä¸€æ¡è‡ªå®šä¹‰æ¶ˆæ¯çš„å›è°ƒã€‚

**ç¬¬1æ­¥ï¼šåˆ›å»º `Callback` å­ç±»**

åœ¨ `core/callbacks.py` ä¸­ï¼ˆæˆ–ä¸€ä¸ªæ–°æ–‡ä»¶ä¸­ï¼‰ï¼Œåˆ›å»ºä¸€ä¸ªæ–°ç±»ã€‚

```python
# in core/callbacks.py
class WelcomeMessage(Callback):
    def on_train_start(self, logs: dict[str, Any] | None = None):
        if self.engine.rank == 0:
            self.engine.logger.info("======================================")
            self.engine.logger.info("ğŸ‰ Welcome to the training session! ğŸ‰")
            self.engine.logger.info("======================================")

    def on_train_end(self, logs: dict[str, Any] | None = None):
        if self.engine.rank == 0:
            self.engine.logger.info("======================================")
            self.engine.logger.info("ğŸ‘‹ Training finished. Goodbye! ğŸ‘‹")
            self.engine.logger.info("======================================")
```

**ç¬¬2æ­¥ï¼šåœ¨ `train.py` ä¸­å®ä¾‹åŒ–å¹¶æ·»åŠ å®ƒ**

åœ¨ `train_worker` å‡½æ•°ä¸­ï¼Œå°†æ‚¨çš„æ–°å›è°ƒæ·»åŠ åˆ° `callbacks` åˆ—è¡¨ä¸­ã€‚

```python
# in train.py
from llm.training.core.callbacks import ..., WelcomeMessage # å¯¼å…¥æ–°å›è°ƒ

def train_worker(...):
    # ...
    callbacks = [
        MetricsLogger(),
        TensorBoardLogger(...),
        LRSchedulerCallback(),
        WelcomeMessage(), # <-- æ·»åŠ æ–°çš„å›è°ƒå®ä¾‹
    ]
    # ...
    engine = TrainingEngine(..., callbacks=callbacks)
    engine.run()
```

ç°åœ¨ï¼Œæ¯æ¬¡è®­ç»ƒè¿è¡Œæ—¶ï¼Œéƒ½ä¼šåœ¨å¼€å§‹å’Œç»“æŸæ—¶çœ‹åˆ°æ‚¨çš„æ¬¢è¿å’Œå‘Šåˆ«æ¶ˆæ¯ã€‚

---

### é£Ÿè°±3ï¼šå¦‚ä½•æ·»åŠ ä¸€ä¸ªå…¨æ–°çš„è®­ç»ƒä»»åŠ¡ï¼Ÿ

è¿™æ˜¯æœ€å¸¸è§çš„æ‰©å±•æ–¹å¼ã€‚å‡è®¾æ‚¨è¦æ·»åŠ ä¸€ä¸ªå›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚

1.  **åˆ›å»ºæ•°æ®æ¨¡å—**: åˆ›å»ºä¸€ä¸ªæ–°çš„ `DataModule` å­ç±»ï¼ˆä¾‹å¦‚ `ImageNetDataModule`ï¼‰ï¼Œè´Ÿè´£ä¸‹è½½ã€é¢„å¤„ç†å’ŒåŠ è½½æ‚¨çš„æ•°æ®ã€‚
2.  **åˆ›å»ºæ¨¡å‹**: åˆ›å»ºä¸€ä¸ªæ–°çš„ `nn.Module`ï¼ˆä¾‹å¦‚ `ResNet`ï¼‰ã€‚
3.  **åˆ›å»ºä»»åŠ¡ç±»**: åˆ›å»ºä¸€ä¸ªæ–°çš„ `TrainingTask` å­ç±»ï¼Œä¾‹å¦‚ `ClassificationTask`ã€‚

    ```python
    # in tasks/classification_task.py
    from .base_task import TrainingTask
    from my_models import ResNet # å‡è®¾æ‚¨å®šä¹‰äº† ResNet

    class ClassificationTask(TrainingTask):
        def build_model(self) -> nn.Module:
            return ResNet(num_classes=self.config.model.num_classes)

        def build_optimizer(self, model: nn.Module) -> optim.Optimizer:
            return optim.SGD(model.parameters(), lr=self.config.training.lr, momentum=0.9)

        def build_scheduler(self, optimizer: optim.Optimizer) -> LRScheduler | None:
            # ...

        def build_criterion(self) -> nn.Module:
            return nn.CrossEntropyLoss()

        def train_step(self, batch, model: nn.Module, criterion: nn.Module) -> tuple[torch.Tensor, dict]:
            images, labels = batch
            outputs = model(images)
            loss = criterion(outputs, labels)
            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(outputs.data, 1)
            accuracy = (predicted == labels).sum().item() / labels.size(0)
            return loss, {"loss": loss.item(), "accuracy": accuracy}

        def validation_step(self, batch, model: nn.Module, criterion: nn.Module) -> tuple[torch.Tensor, dict]:
            # ç±»ä¼¼ train_step çš„é€»è¾‘
            ...
    ```

4.  **åœ¨ `train.py` ä¸­æ³¨å†Œæ–°ä»»åŠ¡**

    ```python
    # in train.py
    from llm.training.tasks.regression_task import RegressionTask
    from llm.training.tasks.classification_task import ClassificationTask # <-- å¯¼å…¥æ–°ä»»åŠ¡

    AVAILABLE_TASKS = {
        "regression": RegressionTask,
        "classification": ClassificationTask, # <-- æ³¨å†Œæ–°ä»»åŠ¡
    }
    ```

ç°åœ¨ï¼Œæ‚¨å¯ä»¥é€šè¿‡è¿è¡Œ `python -m llm.training.train --task classification` æ¥å¯åŠ¨æ‚¨çš„æ–°ä»»åŠ¡ã€‚
