# æŒ‡å—: æ‰©å±•è®­ç»ƒæ¡†æ¶ (`GUIDE_EXTENDING.md`)

æœ¬æ¡†æ¶çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºå…¶å¯æ‰©å±•æ€§ã€‚æœ¬æŒ‡å—å°†é€šè¿‡å‡ ä¸ªâ€œé£Ÿè°±å¼â€çš„ç¤ºä¾‹ï¼Œå‘æ‚¨å±•ç¤ºå¦‚ä½•è½»æ¾åœ°æ·»åŠ æ–°åŠŸèƒ½ã€‚

---

### é£Ÿè°±1: å¦‚ä½•æ·»åŠ ä¸€ä¸ªæ–°çš„å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼Ÿ

å‡è®¾æˆ‘ä»¬æƒ³æ·»åŠ ä¸€ä¸ª `ExponentialLR` è°ƒåº¦å™¨ã€‚

**ç¬¬1æ­¥: åœ¨ `config.py` ä¸­æ·»åŠ é€‰é¡¹**

åœ¨ `TrainingConfig` æ•°æ®ç±»ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä¸º `scheduler_type` æ·»åŠ ä¸€ä¸ªæ–°çš„æœ‰æ•ˆé€‰é¡¹çš„æ³¨é‡Šï¼Œä»¥æ–¹ä¾¿å…¶ä»–å¼€å‘è€…çŸ¥é“å®ƒçš„å­˜åœ¨ã€‚

```python
# in core/config.py
@dataclass
class TrainingConfig:
    # ...
    scheduler_type: str = "cosine"  # æ–°å¢: cosine, step, plateau, exponential
    # ...
```

**ç¬¬2æ­¥: åœ¨ `TrainingTask` ä¸­å®ç°é€»è¾‘**

åœ¨æ‚¨çš„ `TrainingTask` å­ç±»ï¼ˆä¾‹å¦‚ `RegressionTask`ï¼‰çš„ `build_scheduler` æ–¹æ³•ä¸­ï¼Œæ ¹æ®é…ç½®æ·»åŠ å¯¹æ–°è°ƒåº¦å™¨çš„æ”¯æŒã€‚

```python
# in tasks/regression_task.py
from torch.optim.lr_scheduler import ExponentialLR # å¯¼å…¥æ–°çš„è°ƒåº¦å™¨
from torch import optim # å¯¼å…¥ optim

class RegressionTask(TrainingTask):
    # ...
    def build_scheduler(self, optimizer: optim.Optimizer) -> optim.lr_scheduler._LRScheduler | None:
        # æ ¹æ®é…ç½®åŠ¨æ€åˆ›å»ºè°ƒåº¦å™¨
        if self.config.training.scheduler_type == "cosine":
            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self.config.training.epochs)
        elif self.config.training.scheduler_type == "step":
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=self.config.training.step_size, gamma=self.config.training.gamma)
        elif self.config.training.scheduler_type == "plateau":
            scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)
        elif self.config.training.scheduler_type == "exponential": # <-- æ–°å¢é€»è¾‘
            scheduler = ExponentialLR(optimizer, gamma=self.config.training.gamma)
        else:
            scheduler = None # æˆ–è€…æŠ›å‡ºé”™è¯¯

        # ... åç»­çš„ warmup é€»è¾‘ä¿æŒä¸å˜ ...
        return scheduler
```

ç°åœ¨ï¼Œæ‚¨å°±å¯ä»¥é€šè¿‡é…ç½® `--scheduler-type exponential` æ¥ä½¿ç”¨æ–°çš„è°ƒåº¦å™¨äº†ã€‚

---

### é£Ÿè°±2: å¦‚ä½•æ·»åŠ ä¸€ä¸ªæ–°çš„å›è°ƒï¼Ÿ

å‡è®¾æˆ‘ä»¬æƒ³åˆ›å»ºä¸€ä¸ªåœ¨è®­ç»ƒå¼€å§‹å’Œç»“æŸæ—¶æ‰“å°ä¸€æ¡è‡ªå®šä¹‰æ¶ˆæ¯çš„å›è°ƒã€‚

**ç¬¬1æ­¥: åˆ›å»º `Callback` å­ç±»**

åœ¨ `core/callbacks.py` ä¸­ï¼ˆæˆ–ä¸€ä¸ªæ–°æ–‡ä»¶ä¸­ï¼‰ï¼Œåˆ›å»ºä¸€ä¸ªæ–°ç±»ã€‚

```python
# in core/callbacks.py
class WelcomeMessage(Callback):
    def on_train_start(self, logs: dict[str, Any] | None = None):
        if self.engine.rank == 0:
            self.engine.logger.info("======================================")
            self.engine.logger.info("ğŸ‰ Welcome to the training session! ğŸ‰")
            self.engine.logger.info("======================================")

    def on_train_end(self, logs: dict[str, Any] | None = None):
        if self.engine.rank == 0:
            self.engine.logger.info("======================================")
            self.engine.logger.info("ğŸ‘‹ Training finished. Goodbye! ğŸ‘‹")
            self.engine.logger.info("======================================")
```

**ç¬¬2æ­¥: é€šè¿‡é…ç½®åŠ¨æ€æ·»åŠ å›è°ƒ**

ä¸ºäº†ä¿æŒ `train.py` çš„ç®€æ´æ€§å’Œçµæ´»æ€§ï¼Œæˆ‘ä»¬æ¨èé€šè¿‡é…ç½®æ¥åŠ¨æ€æ·»åŠ å›è°ƒã€‚è¿™éœ€è¦æ‚¨åœ¨ `Config` ä¸­å®šä¹‰ä¸€ä¸ªå›è°ƒåˆ—è¡¨ï¼Œå¹¶åœ¨ `train.py` ä¸­æ ¹æ®é…ç½®å®ä¾‹åŒ–å®ƒä»¬ã€‚

**åœ¨ `Config` ä¸­å®šä¹‰å›è°ƒé…ç½® (ç¤ºä¾‹)**:

```python
# in core/config.py
from typing import List

@dataclass
class TrainingConfig:
    # ...
    callbacks: List[str] = field(default_factory=lambda: ["MetricsLogger", "TensorBoardLogger", "LRSchedulerCallback"])
    # ...
```

**åœ¨ `train.py` ä¸­å®ä¾‹åŒ–å›è°ƒ**:

```python
# in train.py
from llm.training.core import callbacks as training_callbacks # å¯¼å…¥å›è°ƒæ¨¡å—

def train_worker(rank: int, world_size: int, config: Config, task_class):
    # ...
    instantiated_callbacks = []
    for cb_name in config.training.callbacks:
        if hasattr(training_callbacks, cb_name):
            cb_class = getattr(training_callbacks, cb_name)
            # æ ¹æ®å›è°ƒç±»å‹ä¼ é€’å¿…è¦çš„å‚æ•°
            if cb_name == "TensorBoardLogger":
                instantiated_callbacks.append(cb_class(log_dir=config.logging.log_dir))
            else:
                instantiated_callbacks.append(cb_class())
        else:
            config.logger.warning(f"Callback {cb_name} not found. Skipping.")

    engine = TrainingEngine(
        config,
        task,
        rank,
        world_size,
        data_module=data_module,
        callbacks=instantiated_callbacks, # ä½¿ç”¨åŠ¨æ€å®ä¾‹åŒ–çš„å›è°ƒ
    )
    engine.run()
```
ç°åœ¨ï¼Œæ‚¨å¯ä»¥é€šè¿‡ä¿®æ”¹ `config.yaml` æˆ–å‘½ä»¤è¡Œå‚æ•°æ¥è½»æ¾æ·»åŠ æˆ–ç§»é™¤å›è°ƒï¼Œä¾‹å¦‚ `--training-callbacks MetricsLogger TensorBoardLogger WelcomeMessage`ã€‚

---

### é£Ÿè°±3: å¦‚ä½•æ·»åŠ ä¸€ä¸ªå…¨æ–°çš„è®­ç»ƒä»»åŠ¡ï¼Ÿ

è¿™æ˜¯æœ€å¸¸è§çš„æ‰©å±•æ–¹å¼ã€‚å‡è®¾æ‚¨è¦æ·»åŠ ä¸€ä¸ªå›¾åƒåˆ†ç±»ä»»åŠ¡ã€‚

1.  **åˆ›å»ºæ•°æ®æ¨¡å—**: åˆ›å»ºä¸€ä¸ªæ–°çš„ `DataModule` å­ç±»ï¼ˆä¾‹å¦‚ `ImageNetDataModule`ï¼‰ï¼Œè´Ÿè´£ä¸‹è½½ã€é¢„å¤„ç†å’ŒåŠ è½½æ‚¨çš„æ•°æ®ã€‚
2.  **åˆ›å»ºæ¨¡å‹**: åˆ›å»ºä¸€ä¸ªæ–°çš„ `nn.Module`ï¼ˆä¾‹å¦‚ `ResNet`ï¼‰ã€‚**è¯·æ³¨æ„ï¼Œæ¨¡å‹åº”æ ¹æ® `Config` ä¸­çš„å‚æ•°è¿›è¡Œæ„å»ºï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç ã€‚**
3.  **åˆ›å»ºä»»åŠ¡ç±»**: åˆ›å»ºä¸€ä¸ªæ–°çš„ `TrainingTask` å­ç±»ï¼Œä¾‹å¦‚ `ClassificationTask`ã€‚

    ```python
    # in tasks/classification_task.py
    from .base_task import TrainingTask
    from my_models import ResNet # å‡è®¾æ‚¨å®šä¹‰äº† ResNet
    from llm.models.decoder import DecoderModel # å¯¼å…¥ DecoderModel

    class ClassificationTask(TrainingTask):
        def build_model(self) -> nn.Module:
            # ç¤ºä¾‹: æ ¹æ®é…ç½®æ„å»ºæ¨¡å‹
            if self.config.model.use_moe:
                # å¦‚æœé…ç½®ä¸­å¯ç”¨äº† MoEï¼Œåˆ™æ„å»ºä¸€ä¸ªå¸¦æœ‰ MoE çš„ DecoderModel
                return DecoderModel(
                    vocab_size=self.config.model.vocab_size, # å‡è®¾ vocab_size åœ¨ ModelConfig ä¸­
                    hidden_size=self.config.model.hidden_size,
                    num_layers=self.config.model.num_layers,
                    num_heads=self.config.model.num_heads,
                    use_moe=self.config.model.use_moe,
                    num_experts=self.config.model.num_experts,
                    top_k=self.config.model.top_k,
                    # ... å…¶ä»–å‚æ•°
                )
            else:
                # å¦åˆ™ï¼Œæ„å»ºä¸€ä¸ªæ ‡å‡†çš„ DecoderModel
                return DecoderModel(
                    vocab_size=self.config.model.vocab_size,
                    hidden_size=self.config.model.hidden_size,
                    num_layers=self.config.model.num_layers,
                    num_heads=self.config.model.num_heads,
                    # ... å…¶ä»–å‚æ•°
                )

        def build_optimizer(self, model: nn.Module) -> optim.Optimizer:
            return optim.SGD(model.parameters(), lr=self.config.training.lr, momentum=0.9)

        def build_scheduler(self, optimizer: optim.Optimizer) -> LRScheduler | None:
            # ...

        def build_criterion(self) -> nn.Module:
            return nn.CrossEntropyLoss()

        def train_step(self, batch, model: nn.Module, criterion: nn.Module) -> tuple[torch.Tensor, dict]:
            images, labels = batch
            outputs = model(images)
            loss = criterion(outputs, labels)
            # è®¡ç®—å‡†ç¡®ç‡
            _, predicted = torch.max(outputs.data, 1)
            accuracy = (predicted == labels).sum().item() / labels.size(0)
            return loss, {"loss": loss.item(), "accuracy": accuracy}

        def validation_step(self, batch, model: nn.Module, criterion: nn.Module) -> tuple[torch.Tensor, dict]:
            # ç±»ä¼¼ train_step çš„é€»è¾‘
            ...
    ```

4.  **åœ¨ `train.py` ä¸­æ³¨å†Œæ–°ä»»åŠ¡**

    ```python
    # in train.py
    from llm.training.tasks.regression_task import RegressionTask
    from llm.training.tasks.classification_task import ClassificationTask # <-- å¯¼å…¥æ–°ä»»åŠ¡

    AVAILABLE_TASKS = {
        "regression": RegressionTask,
        "classification": ClassificationTask, # <-- æ³¨å†Œæ–°ä»»åŠ¡
    }
    ```

ç°åœ¨ï¼Œæ‚¨å¯ä»¥é€šè¿‡è¿è¡Œ `python -m llm.training.train --task classification` æ¥å¯åŠ¨æ‚¨çš„æ–°ä»»åŠ¡ã€‚

---

### é£Ÿè°±4: å¦‚ä½•å¯ç”¨ MoE (Mixture of Experts) åŠŸèƒ½ï¼Ÿ

æœ¬é¡¹ç›®æ¡†æ¶æ”¯æŒ MoE æ¶æ„ï¼Œæ‚¨å¯ä»¥é€šè¿‡é…ç½®è½»æ¾å¯ç”¨å®ƒã€‚

**ç¬¬1æ­¥: åœ¨ `Config` ä¸­é…ç½® MoE å‚æ•°**

åœ¨ `core/config.py` çš„ `ModelConfig` ä¸­ï¼Œè®¾ç½® `use_moe` ä¸º `True`ï¼Œå¹¶æŒ‡å®š `num_experts` å’Œ `top_k`ã€‚

```python
# in config.yaml (æˆ–é€šè¿‡å‘½ä»¤è¡Œå‚æ•°)
model:
  hidden_size: 512
  num_layers: 2
  # ... å…¶ä»–æ¨¡å‹å‚æ•°
  use_moe: true       # å¯ç”¨ MoE
  num_experts: 8      # ä¸“å®¶æ€»æ•°
  top_k: 2            # æ¯ä¸ª token æ¿€æ´»çš„ä¸“å®¶æ•°é‡
```

**ç¬¬2æ­¥: è¿è¡Œè®­ç»ƒ**

å½“æ‚¨è¿è¡Œè®­ç»ƒæ—¶ï¼Œ`TrainingEngine` ä¼šæ ¹æ® `Config` ä¸­çš„è®¾ç½®ï¼Œåœ¨ `TransformerBlock` ä¸­è‡ªåŠ¨å®ä¾‹åŒ– MoE å±‚è€Œä¸æ˜¯æ ‡å‡† MLPã€‚

```bash
python -m llm.training.train --task regression --model-use-moe --model-num-experts 8 --model-top-k 2
```

é€šè¿‡è¿™ç§æ–¹å¼ï¼Œæ‚¨å¯ä»¥è½»æ¾åœ°åœ¨æ¨¡å‹ä¸­å¯ç”¨å’Œé…ç½® MoE åŠŸèƒ½ï¼Œè€Œæ— éœ€ä¿®æ”¹æ ¸å¿ƒæ¨¡å‹ä»£ç ã€‚
