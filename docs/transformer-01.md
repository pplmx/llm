# Transformer æž¶æž„

æœ¬æ–‡ä»¶å±•ç¤ºäº†å®Œæ•´çš„ Transformer æž¶æž„ç»“æž„å›¾ï¼Œä½¿ç”¨ Mermaid ç»˜åˆ¶ï¼Œå¹¶æŒ‰ç…§æ¨¡å—åŒ–ç»“æž„è¿›è¡Œæ‹†åˆ†ã€‚

---

## ðŸ“‹ æž¶æž„å‚æ•°è¯´æ˜Ž

| å‚æ•°               | ç¬¦å·  | è¯´æ˜Ž           |
|------------------|-----|--------------|
| Batch Size       | B   | æ‰¹æ¬¡å¤§å°         |
| Sequence Length  | L   | åºåˆ—é•¿åº¦         |
| Hidden Dimension | H   | éšè—å±‚ç»´åº¦        |
| Number of Heads  | h   | æ³¨æ„åŠ›å¤´æ•°        |
| Head Dimension   | d_k | æ¯ä¸ªå¤´çš„ç»´åº¦ (H/h) |
| FFN Dimension    | 4H  | å‰é¦ˆç½‘ç»œä¸­é—´å±‚ç»´åº¦    |
| Number of Layers | N   | ç¼–ç å™¨/è§£ç å™¨å±‚æ•°    |

---

## ðŸ”¹ Self-Attention æ¨¡å—ï¼ˆSelfAttentionBlockï¼‰

```mermaid
graph TD
    SA_Input["Input X âˆˆ â„^(B,L,H)"] --> SA_LN1[LayerNorm]
    SA_LN1 --> SA_QKV["Linear(Hâ†’3H) â†’ Q,K,V"]
    SA_QKV --> SA_Attn["Multi-Head Attention (QÂ·K^T/âˆšd_k)"]
    SA_Attn --> SA_Drop["Dropout (optional)"]
    SA_Drop --> SA_OutProj["Linear(Hâ†’H)"]
    SA_OutProj --> SA_Drop2["Dropout (optional)"]
    SA_Drop2 --> SA_Residual["+ Residual (Input X)"]
    SA_Residual --> SA_Out[SelfAttention Output]
```

---

## ðŸ”¹ FeedForward æ¨¡å—ï¼ˆFeedForwardBlockï¼‰

```mermaid
graph TD
    FF_Input["Input âˆˆ â„^(B,L,H)"] --> FF_LN[LayerNorm]
    FF_LN --> FF_Linear1["Linear(Hâ†’4H)"]
    FF_Linear1 --> FF_Act["Activation (GELU/ReLU)"]
    FF_Act --> FF_Linear2["Linear(4Hâ†’H)"]
    FF_Linear2 --> FF_Drop["Dropout (optional)"]
    FF_Drop --> FF_Residual["+ Residual (Input)"]
    FF_Residual --> FF_Out[FeedForward Output]
```

---

## ðŸ”¹ MoE æ¨¡å—ï¼ˆMoEBlockï¼‰

```mermaid
graph TD
    MoE_Input["Input âˆˆ â„^(B,L,H)"] --> MoE_LN[LayerNorm]
    MoE_LN --> MoE_Gate["Gating Network (Top-k)"]
    MoE_Gate --> MoE_Dispatch[Dispatch to Experts]
    MoE_Dispatch --> MoE_Expert["Expert FFNs (Linear(Hâ†’4Hâ†’H))"]
    MoE_Expert --> MoE_Combine["Combine Outputs (weighted)"]
    MoE_Combine --> MoE_Drop["Dropout (optional)"]
    MoE_Drop --> MoE_Residual[+ Residual]
    MoE_Residual --> MoE_Out[MoE Output]
```

---

## ðŸ”¹ Transformer Encoder Block

```mermaid
graph TD
    ENC_In["Input X âˆˆ â„^(B,L,H)"] --> SA[SelfAttentionBlock]
    SA --> FF_or_MoE{Use MLP or MoE?}
    FF_or_MoE -->|MLP| FF[FeedForwardBlock]
    FF_or_MoE -->|MoE| MoE[MoEBlock]
    FF --> ENC_Out[Encoder Block Output]
    MoE --> ENC_Out
```

---

## ðŸ”¹ Transformer Decoder Blockï¼ˆå« Cross-Attention + MoEï¼‰

```mermaid
graph TD
    DEC_In["Decoder Input âˆˆ â„^(B,L,H)"] --> D_LN1[LayerNorm]
    D_LN1 --> D_QKV["Linear(Hâ†’3H)"]
    D_QKV --> D_MaskedAttn[Masked Self-Attention]
    D_MaskedAttn --> D_Drop1[Dropout]
    D_Drop1 --> D_Res1[+ Residual]
    D_Res1 --> D_LN2[LayerNorm]
    D_LN2 --> Cross_QKV["Q from decoder, KV from encoder"]
    Cross_QKV --> CrossAttn[Cross-Attention]
    CrossAttn --> D_Drop2[Dropout]
    D_Drop2 --> D_Res2[+ Residual]
    D_Res2 --> D_LN3[LayerNorm]
    D_LN3 --> FF_or_MoE2{Use MLP or MoE?}
    FF_or_MoE2 -->|MLP| FF2[FeedForwardBlock]
    FF_or_MoE2 -->|MoE| MoE2[MoEBlock]
    FF2 --> DEC_Out[Decoder Block Output]
    MoE2 --> DEC_Out
```

---

## ðŸ”¹ ä½ç½®ç¼–ç æ¨¡å—ï¼ˆPositionalEncodingï¼‰

```mermaid
graph TD
    PE_Embed["Token Embedding (B, L, H)"] --> PE_Add[+ Positional Encoding]
    PE_Add --> PE_Out[Input to Transformer]
```

**è¯´æ˜Žï¼šä½ç½®ç¼–ç å¯ä»¥æ˜¯**

- **Sinusoidal**ï¼ˆé™æ€ï¼‰
- **Learned**ï¼ˆå¯è®­ç»ƒï¼‰
- **RoPE**ï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼Œé€‚ç”¨äºŽ QKï¼‰
- **Alibi**ï¼ˆçº¿æ€§åç§» attention logitsï¼‰

---

## ðŸ”¹ Transformer Encoder Stack

```mermaid
graph TD
    Input[Input Tokens] --> PosEnc[PositionalEncoding]
    PosEnc --> E1[Encoder Layer 1]
    E1 --> E2[Encoder Layer 2]
    E2 --> E3[...]
    E3 --> EN[Encoder Layer N]
    EN --> EncoderOutput[Encoder Stack Output]
```

---

## ðŸ”¹ Transformer Decoder Stack

```mermaid
graph TD
    DecInput[Decoder Input] --> PosEncD[PositionalEncoding]
    PosEncD --> D1[Decoder Layer 1]
    D1 --> D2[Decoder Layer 2]
    D2 --> D3[...]
    D3 --> DN[Decoder Layer N]
    DN --> DecoderOutput[Decoder Stack Output]
```

**æ³¨æ„ï¼š** Decoder æ¯å±‚éƒ½è®¿é—® Encoder Stack çš„è¾“å‡ºä½œä¸º Cross-Attention çš„ KVã€‚

---

## ðŸ”¹ å®Œæ•´ Transformer æ¨¡åž‹æž¶æž„

```mermaid
graph TD
    subgraph "Input Processing"
        InputTokens[Input Tokens] --> TokenEmbed[Token Embedding]
        TokenEmbed --> PosEnc1[+ Positional Encoding]
    end

subgraph "Encoder Stack"
PosEnc1 --> EncStack[N Ã— Encoder Layers]
EncStack --> EncOut[Encoder Output]
end

subgraph "Decoder Stack"
TargetTokens[Target Tokens] --> TargetEmbed[Token Embedding]
TargetEmbed --> PosEnc2[+ Positional Encoding]
PosEnc2 --> DecStack[N Ã— Decoder Layers]
EncOut --> DecStack
DecStack --> DecOut[Decoder Output]
end

subgraph "Output Processing"
DecOut --> FinalLN[Final LayerNorm]
FinalLN --> OutputProj[Output Projection]
OutputProj --> Softmax[Softmax]
Softmax --> Probs[Output Probabilities]
end
```
