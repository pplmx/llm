# Usage Guide

This guide provides instructions on how to install, develop, train, and run inference with the `llm` project.

## Installation

To set up the project environment, ensure you have `uv` installed. Then run:

```bash
make init
```

This command initializes the virtual environment and installs pre-commit hooks. Alternatively, you can run:

```bash
uv sync
```

## Development Commands

The project uses `Makefile` to simplify common development tasks.

- **Run Tests**:

  ```bash
  make test
  ```

- **Lint Code**:

  ```bash
  make lint
  ```

- **Format Code**:

  ```bash
  make fmt
  ```

- **Type Check**:

  ```bash
  make ty
  ```

- **View Allure Report**:

  ```bash
  make allure
  ```

- **Clean Build Artifacts**:

  ```bash
  make clean
  ```

## Training

### Using the Training Script

The main training entry point is `src/llm/training/train.py`. You can run it using `uv run`.

Syntax:

```bash
uv run -m llm.training.train --task <task_name> [options]
```

Use `--help` to see all available options dynamically generated by Typer:

```bash
uv run -m llm.training.train --help
```

Example (Regression Task):

```bash
uv run -m llm.training.train --task regression --epochs 10
```

### Standalone Simple Decoder Training

For a simple example of training a decoder-only model on a text file, use `scripts/train_simple_decoder.py`.

```bash
uv run scripts/train_simple_decoder.py --file-path <path_to_text_file> [options]
```

**Common Options:**

- `--file-path`: Path to the training text file (Required).
- `--val-file-path`: Path to the validation text file.
- `--device`: `cpu` or `cuda`.
- `--epochs`: Number of training epochs.
- `--batch-size`: Batch size.

Example:

```bash
uv run scripts/train_simple_decoder.py --file-path data/dummy_corpus.txt --epochs 5
```

The script will automatically use CUDA if available, otherwise it falls back to CPU. You can force a specific device using `--device`:

## Inference

To generate text using a trained model, you can use the `generate` function from `src/llm/inference.py`.

### Python API Example

```python
import torch
from llm.models.decoder import DecoderModel
from llm.tokenization.tokenizer import HFTokenizer
from llm.inference import generate

# 1. Load Tokenizer (e.g., GPT-2 from HuggingFace)
tokenizer = HFTokenizer.from_pretrained("gpt2")

# 2. Initialize Model
# Configuration should match the tokenizer's vocab size
model = DecoderModel(
    vocab_size=tokenizer.vocab_size,
    hidden_size=64,
    num_layers=2,
    num_heads=4,
    max_seq_len=128,
)
# Load weights here if available
# model.load_state_dict(torch.load("path/to/model.pt"))

# 3. Generate Text
generated_text = generate(
    model=model,
    tokenizer=tokenizer,
    prompt="Hello",
    max_new_tokens=20,
    temperature=0.8
)
print(generated_text)
```

## Inference Serving

This project includes a production-ready REST API for inference service, built with FastAPI.

### Features

- **Streaming Support**: Server-Sent Events (SSE) for real-time token generation.
- **Advanced Sampling**: Support for `top_p` (Nucleus Sampling) and `repetition_penalty`.
- **Production Ready**: Structured logging, Prometheus metrics, and API Key authentication.

### Starting the Server

**Using Docker (Recommended):**

```bash
make image
make compose-up
```

**Local Development:**

```bash
python src/llm/serving/api.py
```

Or using `uv`:

```bash
uv run src/llm/serving/api.py
```

### API Usage

#### POST /generate

Generate text from a prompt.

**Request Body:**

```json
{
  "prompt": "Hello, world",
  "max_new_tokens": 50,
  "temperature": 0.8,
  "top_k": 5,
  "top_p": 0.9,
  "repetition_penalty": 1.1,
  "stream": false
}
```

**Streaming Request:**

Set `"stream": true` to receive a stream of tokens.

```bash
curl -X POST "http://127.0.0.1:8000/generate" \
     -H "Content-Type: application/json" \
     -d '{"prompt": "Tell me a story", "stream": true}'
```

**Response (Non-streaming):**

```json
{
  "generated_text": "Hello, world! This is a generated text...",
  "token_count": 12
}
```

### Authentication

If `LLM_SERVING_API_KEY` is set, you must provide the key in the `X-API-Key` header.

```bash
export LLM_SERVING_API_KEY="my-secret-key"
# Start server...

curl -X POST "http://127.0.0.1:8000/generate" \
     -H "X-API-Key: my-secret-key" \
     ...
```

### Configuration

You can configure the serving engine using environment variables:

| Variable | Description | Default |
|----------|-------------|---------|
| `LLM_SERVING_MODEL_PATH` | Path to model checkpoint file | `None` (Dummy Model) |
| `LLM_SERVING_DEVICE` | Computation device (`cpu`, `cuda`, `auto`) | `auto` |
| `LLM_SERVING_API_KEY` | API Key for authentication | `None` (Disabled) |
| `LLM_SERVING_LOG_LEVEL` | Logging level (`INFO`, `DEBUG`, etc.) | `INFO` |

### Metrics

Prometheus metrics are available at `/metrics`.

```bash
curl http://127.0.0.1:8000/metrics
```

### Performance Benchmarking

A benchmark script is provided to measure inference performance (Latency and TPS).

```bash
# Run benchmark with torch.compile enabled
uv run scripts/benchmark_inference.py --compile --runs 5
```

Arguments:

- `--runs`: Number of benchmark iterations.
- `--compile`: Enable `torch.compile` optimization.
- `--device`: Target device (e.g., `cuda`).
- `--max_new_tokens`: Number of tokens to generate per run.
