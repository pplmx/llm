"""
CPUå‹å¥½ç‰ˆSFTæ•™å­¦ä»£ç  (v5.0)
é€‚é…æ¡ä»¶ï¼šIntel i7-12700 + 64GBå†…å­˜
è®­ç»ƒè€—æ—¶é¢„ä¼°ï¼šçº¦2å°æ—¶/epoch (1000æ¡æ•°æ®)
"""

import os

import torch
from datasets import Dataset
from peft import LoraConfig, get_peft_model
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
    Trainer,
    TrainingArguments,
)

# ğŸ–¥ï¸ ç¡¬ä»¶é…ç½®
DEVICE = "cpu"
TORCH_DTYPE = torch.bfloat16 if torch.cuda.is_available() else torch.float32
os.environ["OMP_NUM_THREADS"] = "12"  # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´

# ğŸ§© æ¨¡å‹é€‰æ‹© (TinyLlamaä¸­æ–‡è£å‰ªç‰ˆ)
MODEL_NAME = "uer/gpt2-distil-chinese-cluecorpussmall"  # ä»…280MB

# ğŸ“¦ åˆå§‹åŒ–ç»„ä»¶
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=TORCH_DTYPE)

# ğŸ› ï¸ å¤„ç†ç‰¹æ®Štoken (é‡è¦ï¼)
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({"pad_token": "[PAD]"})
model.resize_token_embeddings(len(tokenizer))

# ğŸ›ï¸ LoRAé…ç½® (å‡å°‘90%è®­ç»ƒå‚æ•°)
lora_config = LoraConfig(
    r=8,  # ä½ç§©ç»´åº¦
    lora_alpha=16,
    target_modules=["c_attn"],  # ä»…å¾®è°ƒå…³é”®å±‚
    lora_dropout=0.05,
    task_type="CAUSAL_LM",
    fan_in_fan_out=True,
)
model = get_peft_model(model, lora_config)
model.config.problem_type = "text-generation"
model.print_trainable_parameters()  # æ˜¾ç¤ºå¯è®­ç»ƒå‚æ•°

# ğŸ“š åˆ›å»ºæ•™å­¦æ•°æ®é›† (å°æ ·æœ¬æ¼”ç¤º)
train_texts = [
    "é—®é¢˜ï¼š1+1ç­‰äºå‡ ï¼Ÿå›ç­”ï¼š1+1ç­‰äº2ã€‚",
    "é—®é¢˜ï¼šä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿå›ç­”ï¼šåŒ—äº¬ã€‚",
    "é—®é¢˜ï¼šè°å‘æ˜äº†ç”µè¯ï¼Ÿå›ç­”ï¼šäºšå†å±±å¤§Â·è´å°”ã€‚",
] * 50  # å…±150æ¡æ•°æ®


# âœ‚ï¸ æ•°æ®é¢„å¤„ç†
def tokenize_fn(examples):
    return tokenizer(examples["text"], max_length=128, padding="max_length", truncation=True, return_tensors="pt")


dataset = Dataset.from_dict({"text": train_texts})
dataset = dataset.map(tokenize_fn, batched=True, remove_columns=["text"])

# ğŸ¯ è®­ç»ƒå‚æ•°ä¼˜åŒ– (CPUä¸“ç”¨)
training_args = TrainingArguments(
    output_dir="./cpu_sft",
    per_device_train_batch_size=2,  # æ ¹æ®å†…å­˜è°ƒæ•´
    gradient_accumulation_steps=8,  # æ¨¡æ‹Ÿbatch_size=16
    num_train_epochs=3,
    learning_rate=1e-4,
    weight_decay=0.01,
    logging_steps=10,
    optim="adamw_torch",
    report_to="none",
    use_cpu=True,
    use_mps_device=False,
)

# ğŸ§  æ•°æ®æ•´ç†å™¨
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,  # å› æœè¯­è¨€æ¨¡å‹
)

# ğŸš€ å¼€å§‹è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=data_collator,
)

print("å¼€å§‹è®­ç»ƒ...")
trainer.train()

# ğŸ§ª æµ‹è¯•æ¨ç†
test_prompts = ["é—®é¢˜ï¼š1+1ç­‰äºå‡ ï¼Ÿå›ç­”ï¼š", "é—®é¢˜ï¼šåŒ—äº¬æœ‰å“ªäº›è‘—åæ™¯ç‚¹ï¼Ÿå›ç­”ï¼š"]

for prompt in test_prompts:
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(
        inputs.input_ids, max_new_tokens=50, do_sample=True, temperature=0.9, pad_token_id=tokenizer.pad_token_id
    )
    print(f"è¾“å…¥ï¼š{prompt}\nè¾“å‡ºï¼š{tokenizer.decode(outputs[0], skip_special_tokens=True)}\n")
