# Transformer æž¶æž„

æœ¬æ–‡ä»¶å±•ç¤ºäº†å®Œæ•´çš„ Transformer æž¶æž„ç»“æž„å›¾, ä½¿ç”¨ Mermaid ç»˜åˆ¶, å¹¶æŒ‰ç…§æ¨¡å—åŒ–ç»“æž„è¿›è¡Œæ‹†åˆ†.

---

## ðŸ“‹ æž¶æž„å‚æ•°è¯´æ˜Ž

| å‚æ•°               | ç¬¦å·  | è¯´æ˜Ž           |
|------------------|-----|--------------|
| Batch Size       | B   | æ‰¹æ¬¡å¤§å°         |
| Sequence Length  | L   | åºåˆ—é•¿åº¦         |
| Hidden Dimension | H   | éšè—å±‚ç»´åº¦        |
| Number of Heads  | h   | æ³¨æ„åŠ›å¤´æ•°        |
| Head Dimension   | d_k | æ¯ä¸ªå¤´çš„ç»´åº¦ (H/h) |
| FFN Dimension    | 4H  | å‰é¦ˆç½‘ç»œä¸­é—´å±‚ç»´åº¦    |
| Number of Layers | N   | ç¼–ç å™¨/è§£ç å™¨å±‚æ•°    |

---

## ðŸ”¹ Self-Attention æ¨¡å—(SelfAttentionBlock)

```mermaid
graph TD
    SA_Input["Input X âˆˆ â„^(B,L,H)"] --> SA_LN1[LayerNorm]
    SA_LN1 --> SA_QKV["Linear(Hâ†’3H) â†’ Q,K,V"]
    SA_QKV --> SA_Attn["Multi-Head Attention (QÂ·K^T/âˆšd_k)"]
    SA_Attn --> SA_Drop["Dropout (optional)"]
    SA_Drop --> SA_OutProj["Linear(Hâ†’H)"]
    SA_OutProj --> SA_Drop2["Dropout (optional)"]
    SA_Drop2 --> SA_Residual["+ Residual (Input X)"]
    SA_Residual --> SA_Out[SelfAttention Output]
```

---

## ðŸ”¹ Attention å˜ä½“å¯¹æ¯” (MHA vs GQA vs MQA)

### 1. Multi-Head Attention (MHA)

æ ‡å‡† Transformer ä½¿ç”¨ã€‚æ¯ä¸ª Query Head éƒ½æœ‰å¯¹åº”çš„ Key/Value Headã€‚

```mermaid
graph TD
    subgraph "MHA (h=4)"
        Q1[Q1] --- K1[K1] & V1[V1]
        Q2[Q2] --- K2[K2] & V2[V2]
        Q3[Q3] --- K3[K3] & V3[V3]
        Q4[Q4] --- K4[K4] & V4[V4]
    end
    style Q1 fill:#e3f2fd
    style K1 fill:#fff3e0
```

### 2. Grouped Query Attention (GQA)

LLaMA 2/3 ä½¿ç”¨ã€‚å¤šä¸ª Query Head å…±äº«ä¸€ç»„ Key/Value Headã€‚
(ä¾‹å¦‚: 4ä¸ª Query Head, 2ç»„ KV Head -> 2 Q share 1 KV)

```mermaid
graph TD
    subgraph "GQA (h=4, g=2)"
        Q1[Q1] --> KV1[KV1]
        Q2[Q2] --> KV1
        Q3[Q3] --> KV2[KV2]
        Q4[Q4] --> KV2
    end
    style Q1 fill:#e3f2fd
    style KV1 fill:#fff3e0
```

### 3. Multi-Query Attention (MQA)

æžè‡´æ˜¾å­˜ä¼˜åŒ–ã€‚æ‰€æœ‰ Query Head å…±äº«åŒä¸€ç»„ Key/Value Headã€‚

```mermaid
graph TD
    subgraph "MQA (h=4, g=1)"
        Q1[Q1] --> KV[KV Shared]
        Q2[Q2] --> KV
        Q3[Q3] --> KV
        Q4[Q4] --> KV
    end
    style Q1 fill:#e3f2fd
    style KV fill:#dcedc8
```

---

## ðŸ”¹ FeedForward æ¨¡å—(FeedForwardBlock)

```mermaid
graph TD
    FF_Input["Input âˆˆ â„^(B,L,H)"] --> FF_LN[LayerNorm]
    FF_LN --> FF_Linear1["Linear(Hâ†’4H)"]
    FF_Linear1 --> FF_Act["Activation (GELU/ReLU)"]
    FF_Act --> FF_Linear2["Linear(4Hâ†’H)"]
    FF_Linear2 --> FF_Drop["Dropout (optional)"]
    FF_Drop --> FF_Residual["+ Residual (Input)"]
    FF_Residual --> FF_Out[FeedForward Output]
```

---

## ðŸ”¹ MoE æ¨¡å—(MoEBlock)

```mermaid
graph TD
    MoE_Input["Input âˆˆ â„^(B,L,H)"] --> MoE_LN[LayerNorm]
    MoE_LN --> MoE_Gate["Gating Network (Top-k)"]
    MoE_Gate --> MoE_Dispatch[Dispatch to Experts]
    MoE_Dispatch --> MoE_Expert["Expert FFNs (Linear(Hâ†’4Hâ†’H))"]
    MoE_Expert --> MoE_Combine["Combine Outputs (weighted)"]
    MoE_Combine --> MoE_Drop["Dropout (optional)"]
    MoE_Drop --> MoE_Residual[+ Residual]
    MoE_Residual --> MoE_Out[MoE Output]
```

---

## ðŸ”¹ Transformer Encoder Block

```mermaid
graph TD
    ENC_In["Input X âˆˆ â„^(B,L,H)"] --> SA[SelfAttentionBlock]
    SA --> FF_or_MoE{Use MLP or MoE?}
    FF_or_MoE -->|MLP| FF[FeedForwardBlock]
    FF_or_MoE -->|MoE| MoE[MoEBlock]
    FF --> ENC_Out[Encoder Block Output]
    MoE --> ENC_Out
```

---

## ðŸ”¹ Transformer Decoder Block(å« Cross-Attention + MoE)

```mermaid
graph TD
    DEC_In["Decoder Input âˆˆ â„^(B,L,H)"] --> D_LN1[LayerNorm]
    D_LN1 --> D_QKV["Linear(Hâ†’3H)"]
    D_QKV --> D_MaskedAttn[Masked Self-Attention]
    D_MaskedAttn --> D_Drop1[Dropout]
    D_Drop1 --> D_Res1[+ Residual]
    D_Res1 --> D_LN2[LayerNorm]
    D_LN2 --> Cross_QKV["Q from decoder, KV from encoder"]
    Cross_QKV --> CrossAttn[Cross-Attention]
    CrossAttn --> D_Drop2[Dropout]
    D_Drop2 --> D_Res2[+ Residual]
    D_Res2 --> D_LN3[LayerNorm]
    D_LN3 --> FF_or_MoE2{Use MLP or MoE?}
    FF_or_MoE2 -->|MLP| FF2[FeedForwardBlock]
    FF_or_MoE2 -->|MoE| MoE2[MoEBlock]
    FF2 --> DEC_Out[Decoder Block Output]
    MoE2 --> DEC_Out
```

---

## ðŸ”¹ ä½ç½®ç¼–ç æ¨¡å—(PositionalEncoding)

```mermaid
graph TD
    PE_Embed["Token Embedding (B, L, H)"] --> PE_Add[+ Positional Encoding]
    PE_Add --> PE_Out[Input to Transformer]
```

**è¯´æ˜Ž: ä½ç½®ç¼–ç å¯ä»¥æ˜¯**

- **Sinusoidal**(é™æ€)
- **Learned**(å¯è®­ç»ƒ)
- **RoPE**(æ—‹è½¬ä½ç½®ç¼–ç , é€‚ç”¨äºŽ QK)
- **Alibi**(çº¿æ€§åç§» attention logits)

---

## ðŸ”¹ Transformer Encoder Stack

```mermaid
graph TD
    Input[Input Tokens] --> PosEnc[PositionalEncoding]
    PosEnc --> E1[Encoder Layer 1]
    E1 --> E2[Encoder Layer 2]
    E2 --> E3[...]
    E3 --> EN[Encoder Layer N]
    EN --> EncoderOutput[Encoder Stack Output]
```

---

## ðŸ”¹ Transformer Decoder Stack

```mermaid
graph TD
    DecInput[Decoder Input] --> PosEncD[PositionalEncoding]
    PosEncD --> D1[Decoder Layer 1]
    D1 --> D2[Decoder Layer 2]
    D2 --> D3[...]
    D3 --> DN[Decoder Layer N]
    DN --> DecoderOutput[Decoder Stack Output]
```

**æ³¨æ„:** Decoder æ¯å±‚éƒ½è®¿é—® Encoder Stack çš„è¾“å‡ºä½œä¸º Cross-Attention çš„ KV.

---

## ðŸ”¹ å®Œæ•´ Transformer æ¨¡åž‹æž¶æž„

```mermaid
graph TD
    subgraph "Input Processing"
        InputTokens[Input Tokens] --> TokenEmbed[Token Embedding]
        TokenEmbed --> PosEnc1[+ Positional Encoding]
    end

subgraph "Encoder Stack"
PosEnc1 --> EncStack[N Ã— Encoder Layers]
EncStack --> EncOut[Encoder Output]
end

subgraph "Decoder Stack"
TargetTokens[Target Tokens] --> TargetEmbed[Token Embedding]
TargetEmbed --> PosEnc2[+ Positional Encoding]
PosEnc2 --> DecStack[N Ã— Decoder Layers]
EncOut --> DecStack
DecStack --> DecOut[Decoder Output]
end

subgraph "Output Processing"
DecOut --> FinalLN[Final LayerNorm]
FinalLN --> OutputProj[Output Projection]
OutputProj --> Softmax[Softmax]
Softmax --> Probs[Output Probabilities]
end
```
