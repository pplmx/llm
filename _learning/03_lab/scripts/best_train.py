import argparse
import logging
import os
import shutil
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Union, get_args, get_origin

import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn as nn
import torch.optim as optim
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.optim.lr_scheduler import LRScheduler
from torch.utils.data import DataLoader, DistributedSampler, TensorDataset

# ============================================================================
# 1. é…ç½®ç®¡ç† (Configuration)
# ============================================================================


@dataclass
class Config:
    """ä¸»é…ç½®ç±», æ•´åˆæ‰€æœ‰å‚æ•°"""

    # æ¨¡å‹é…ç½®
    hidden_size: int = 512
    ffn_hidden_size: int | None = None  # è‹¥ä¸ºNone, åˆ™è‡ªåŠ¨è®¾ç½®ä¸º hidden_size * 4
    num_layers: int = 2
    dropout: float = 0.1

    # è®­ç»ƒé…ç½®
    batch_size: int = 128
    epochs: int = 10
    lr: float = 1e-3
    weight_decay: float = 0.01
    gradient_clip_val: float = 1.0
    num_samples: int = 20000

    # ä¼˜åŒ–é…ç½®
    use_amp: bool = True
    use_compile: bool = True
    num_workers: int = 4
    pin_memory: bool = True

    # æ£€æŸ¥ç‚¹é…ç½®
    checkpoint_dir: str = "checkpoints"
    resume_from_checkpoint: str | None = None
    save_interval: int = 1
    keep_last_n: int = 3
    save_best: bool = True

    # æ—¥å¿—é…ç½®
    log_interval: int = 50  # æ¯Nä¸ªbatchè®°å½•ä¸€æ¬¡

    # åˆ†å¸ƒå¼é…ç½® (ä¸»è¦é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®)
    backend: str = "nccl"
    master_addr: str = field(init=False)
    master_port: str = field(init=False)
    world_size: int = field(init=False)
    rank: int = field(init=False)

    def __post_init__(self):
        """åœ¨åˆå§‹åŒ–åè¿›è¡Œä¾èµ–è®¡ç®—å’ŒéªŒè¯"""
        if self.ffn_hidden_size is None:
            self.ffn_hidden_size = self.hidden_size * 4

    @classmethod
    def from_args(cls) -> "Config":
        """ä»å‘½ä»¤è¡Œå‚æ•°åˆ›å»ºé…ç½®, å¹¶ä»ç¯å¢ƒå˜é‡åŠ è½½åˆ†å¸ƒå¼è®¾ç½®"""
        parser = argparse.ArgumentParser(description="An elegant PyTorch DDP Training Script")
        # ä»Configç±»çš„å­—æ®µåŠ¨æ€æ·»åŠ å‘½ä»¤è¡Œå‚æ•°
        for name, default in cls.__annotations__.items():
            if name in ["master_addr", "master_port", "world_size", "rank"]:
                continue  # è¿™äº›ä»ç¯å¢ƒå˜é‡è·å–
            arg_name = f"--{name.replace('_', '-')}"
            try:
                # å¤„ç† Optional[type] çš„æƒ…å†µ
                arg_type = get_origin(default) or default
                if get_origin(arg_type) is Union:
                    arg_type = get_args(arg_type)[0]

                # ä¸ºå¸ƒå°”å€¼è®¾ç½® action
                if arg_type is bool:
                    parser.add_argument(arg_name, action="store_true", default=None)
                else:
                    parser.add_argument(arg_name, type=arg_type)
            except (TypeError, AttributeError):
                continue  # è·³è¿‡æ— æ³•è§£æçš„ç±»å‹

        args = parser.parse_args()

        # åˆ›å»ºé»˜è®¤é…ç½®å®ä¾‹
        config = cls()

        # ç”¨å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é»˜è®¤å€¼
        for name, value in vars(args).items():
            if value is not None:
                setattr(config, name, value)

        # ä»ç¯å¢ƒå˜é‡åŠ è½½åˆ†å¸ƒå¼é…ç½®
        config.master_addr = os.environ.get("MASTER_ADDR", "127.0.0.1")
        config.master_port = os.environ.get("MASTER_PORT", "12355")
        config.world_size = int(os.environ.get("WORLD_SIZE", torch.cuda.device_count()))
        config.rank = int(os.environ.get("RANK", -1))  # -1 è¡¨ç¤ºéåˆ†å¸ƒå¼ç¯å¢ƒ

        return config


# ============================================================================
# 2. æ—¥å¿—ä¸åˆ†å¸ƒå¼å·¥å…· (Utilities)
# ============================================================================


def setup_logger(rank: int) -> logging.Logger:
    """é…ç½®æ—¥å¿—è®°å½•å™¨"""
    logger = logging.getLogger(f"rank_{rank}")
    logger.setLevel(logging.INFO)
    formatter = logging.Formatter(f"[%(asctime)s][Rank {rank}] %(message)s", datefmt="%H:%M:%S")

    if rank == 0:
        handler = logging.StreamHandler()
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    else:
        logger.addHandler(logging.NullHandler())
    return logger


def setup_distributed(rank: int, world_size: int, config: Config):
    """åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ"""
    os.environ["MASTER_ADDR"] = config.master_addr
    os.environ["MASTER_PORT"] = config.master_port
    dist.init_process_group(backend=config.backend, rank=rank, world_size=world_size)
    torch.cuda.set_device(rank)
    torch.manual_seed(42 + rank)  # ä¿è¯æ¯ä¸ªè¿›ç¨‹æœ‰ä¸åŒçš„éšæœºç§å­


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


# ============================================================================
# 3. æ¨¡å‹å®šä¹‰ (Model)
# ============================================================================


class SimpleMLP(nn.Module):
    """æ›´å…·ä»£è¡¨æ€§çš„å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹"""

    def __init__(self, config: Config):
        super().__init__()
        layers = []
        input_size = config.hidden_size
        for i in range(config.num_layers):
            is_last_layer = i == config.num_layers - 1
            output_size = config.hidden_size if is_last_layer else config.ffn_hidden_size
            layers.append(nn.Linear(input_size, output_size))
            if not is_last_layer:
                layers.append(nn.GELU())  # ä½¿ç”¨æ›´ç°ä»£çš„æ¿€æ´»å‡½æ•°
                layers.append(nn.Dropout(config.dropout))
            input_size = output_size
        self.net = nn.Sequential(*layers)
        self._init_weights()

    def _init_weights(self):
        for module in self.modules():
            if isinstance(module, nn.Linear):
                nn.init.xavier_uniform_(module.weight)
                if module.bias is not None:
                    nn.init.zeros_(module.bias)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)


# ============================================================================
# 4. æ£€æŸ¥ç‚¹ç®¡ç† (Checkpointing)
# ============================================================================


class CheckpointManager:
    def __init__(self, config: Config, rank: int, logger: logging.Logger):
        self.config = config
        self.rank = rank
        self.logger = logger
        self.best_metric = float("inf")
        self.saved_checkpoints = []
        if self.rank == 0:
            Path(self.config.checkpoint_dir).mkdir(parents=True, exist_ok=True)

    def save(
        self,
        epoch: int,
        metric: float,
        model: DDP,
        optimizer: optim.Optimizer,
        scheduler: LRScheduler,
        scaler: torch.amp.GradScaler,
    ):
        if self.rank != 0:
            return

        state = {
            "epoch": epoch,
            "metric": metric,
            "best_metric": self.best_metric,
            "model_state": model.module.state_dict(),
            "optimizer_state": optimizer.state_dict(),
            "scheduler_state": scheduler.state_dict(),
            "scaler_state": scaler.state_dict(),
        }

        def atomic_save(data: dict[str, Any], path: Path):
            temp_path = path.with_suffix(".tmp")
            torch.save(data, temp_path)
            shutil.move(temp_path, path)

        # 1. ä¿å­˜å‘¨æœŸæ€§æ£€æŸ¥ç‚¹
        if (epoch + 1) % self.config.save_interval == 0:
            epoch_path = Path(self.config.checkpoint_dir) / f"epoch_{epoch + 1}.pt"
            atomic_save(state, epoch_path)
            self.saved_checkpoints.append(epoch_path)
            self._cleanup_old_checkpoints()
            self.logger.info(f"ğŸ’¾ Checkpoint saved to {epoch_path}")

        # 2. ä¿å­˜æœ€ä½³æ¨¡å‹
        if self.config.save_best and metric < self.best_metric:
            self.best_metric = metric
            state["best_metric"] = self.best_metric
            best_path = Path(self.config.checkpoint_dir) / "best.pt"
            atomic_save(state, best_path)
            self.logger.info(f"ğŸ† New best model saved with metric {metric:.4f}")

    def _cleanup_old_checkpoints(self):
        while len(self.saved_checkpoints) > self.config.keep_last_n:
            oldest = self.saved_checkpoints.pop(0)
            if oldest.exists():
                oldest.unlink()

    def load(
        self,
        model: nn.Module,
        optimizer: optim.Optimizer,
        scheduler: LRScheduler,
        scaler: torch.amp.GradScaler,
        device: torch.device,
    ) -> int:
        if not self.config.resume_from_checkpoint:
            return 0

        ckp_path = Path(self.config.resume_from_checkpoint)
        if not ckp_path.exists():
            self.logger.warning(f"Checkpoint file not found: {ckp_path}. Starting from scratch.")
            return 0

        try:
            checkpoint = torch.load(ckp_path, map_location=device)
            model.load_state_dict(checkpoint["model_state"])
            optimizer.load_state_dict(checkpoint["optimizer_state"])
            scheduler.load_state_dict(checkpoint["scheduler_state"])
            scaler.load_state_dict(checkpoint["scaler_state"])
            self.best_metric = checkpoint.get("best_metric", float("inf"))
            start_epoch = checkpoint["epoch"] + 1
            self.logger.info(f"âœ… Resumed from epoch {start_epoch} using {ckp_path}")
            return start_epoch
        except Exception as e:
            self.logger.error(f"Failed to load checkpoint from {ckp_path}: {e}. Starting from scratch.")
            return 0


# ============================================================================
# 5. è®­ç»ƒå™¨ (Trainer)
# ============================================================================


class Trainer:
    def __init__(self, config: Config, rank: int, world_size: int):
        self.config = config
        self.rank = rank
        self.world_size = world_size
        self.device = torch.device(f"cuda:{rank}")
        self.logger = setup_logger(rank)

        # 1. åˆå§‹åŒ–æ¨¡å‹
        model = SimpleMLP(self.config).to(self.device)

        # 2. åˆå§‹åŒ–ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ã€AMP Scaler
        self.optimizer = optim.AdamW(
            model.parameters(), lr=self.config.lr, weight_decay=self.config.weight_decay, fused=True
        )
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.config.epochs)
        self.scaler = torch.amp.GradScaler(enabled=self.config.use_amp)
        self.criterion = nn.MSELoss()

        # 3. åŠ è½½æ£€æŸ¥ç‚¹(åœ¨DDPåŒ…è£…å‰)
        self.checkpoint_manager = CheckpointManager(config, rank, self.logger)
        self.start_epoch = self.checkpoint_manager.load(model, self.optimizer, self.scheduler, self.scaler, self.device)

        # 4. ç¼–è¯‘æ¨¡å‹(åœ¨DDPåŒ…è£…å‰)
        if self.config.use_compile:
            self.logger.info("ğŸš€ Compiling model with torch.compile...")
            model = torch.compile(model, mode="reduce-overhead")

        # 5. DDPåŒ…è£…
        self.model = DDP(model, device_ids=[self.rank])

        # 6. å‡†å¤‡æ•°æ®
        self.dataloader, self.sampler = self._create_dataloader()

    def _create_dataloader(self) -> tuple[DataLoader, DistributedSampler]:
        x = torch.randn(self.config.num_samples, self.config.hidden_size)
        y = torch.randn_like(x)
        dataset = TensorDataset(x, y)
        sampler = DistributedSampler(dataset, num_replicas=self.world_size, rank=self.rank, shuffle=True)
        return DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            sampler=sampler,
            num_workers=self.config.num_workers,
            pin_memory=self.config.pin_memory,
            persistent_workers=self.config.num_workers > 0,
        ), sampler

    def _run_epoch(self, epoch: int) -> float:
        self.sampler.set_epoch(epoch)
        self.model.train()
        total_loss = 0.0

        for batch_idx, (data, target) in enumerate(self.dataloader):
            data = data.to(self.device, non_blocking=True)
            target = target.to(self.device, non_blocking=True)

            with torch.amp.autocast(device_type="cuda", dtype=torch.bfloat16, enabled=self.config.use_amp):
                output = self.model(data)
                loss = self.criterion(output, target)

            self.optimizer.zero_grad(set_to_none=True)
            self.scaler.scale(loss).backward()

            # Unscale and clip gradients
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.gradient_clip_val)

            self.scaler.step(self.optimizer)
            self.scaler.update()

            total_loss += loss.item()

            if self.rank == 0 and (batch_idx + 1) % self.config.log_interval == 0:
                mem = torch.cuda.memory_allocated(self.device) / 1024**3
                self.logger.info(
                    f"Epoch {epoch + 1:2d} | Batch {batch_idx + 1:4d}/{len(self.dataloader)} | "
                    f"Loss: {loss.item():.4f} | Mem (GB): {mem:.2f}"
                )

        return total_loss / len(self.dataloader)

    def train(self):
        if self.rank == 0:
            self.logger.info("ğŸ‰ Starting training...")

        for epoch in range(self.start_epoch, self.config.epochs):
            epoch_start_time = time.time()

            avg_loss = self._run_epoch(epoch)

            # åœ¨æ‰€æœ‰è¿›ç¨‹ä¸ŠåŒæ­¥å¹³å‡æŸå¤±, ä»¥ä¾¿è¿›è¡Œä¸€è‡´çš„å†³ç­–
            loss_tensor = torch.tensor(avg_loss, device=self.device)
            dist.all_reduce(loss_tensor, op=dist.ReduceOp.AVG)
            global_avg_loss = loss_tensor.item()

            self.scheduler.step()
            dist.barrier()  # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆepoch

            if self.rank == 0:
                epoch_time = time.time() - epoch_start_time
                lr = self.scheduler.get_last_lr()[0]
                self.logger.info(
                    f"Epoch {epoch + 1:2d} SUMMARY | Avg Loss: {global_avg_loss:.4f} | "
                    f"LR: {lr:.6f} | Time: {epoch_time:.2f}s"
                )
                self.checkpoint_manager.save(
                    epoch, global_avg_loss, self.model, self.optimizer, self.scheduler, self.scaler
                )

        if self.rank == 0:
            self.logger.info(f"âœ… Training completed. Best metric: {self.checkpoint_manager.best_metric:.4f}")


# ============================================================================
# 6. ä¸»ç¨‹åºå…¥å£ (Main)
# ============================================================================


def worker_fn(rank: int, world_size: int, config: Config):
    """æ¯ä¸ªDDPè¿›ç¨‹çš„æ‰§è¡Œå‡½æ•°"""
    try:
        setup_distributed(rank, world_size, config)
        trainer = Trainer(config, rank, world_size)
        trainer.train()
    except Exception:
        logging.getLogger(f"rank_{rank}").exception("An error occurred during training.")
        raise
    finally:
        cleanup_distributed()


def main():
    """ä¸»å‡½æ•°, è´Ÿè´£å¯åŠ¨è®­ç»ƒ"""
    config = Config.from_args()

    # è®¾ç½®PyTorchæ€§èƒ½é€‰é¡¹
    torch.backends.cudnn.benchmark = True
    torch.set_float32_matmul_precision("high")

    world_size = config.world_size

    # ä¸´æ—¶åˆ›å»ºrank 0çš„æ—¥å¿—å™¨ç”¨äºä¸»è¿›ç¨‹ä¿¡æ¯æ‰“å°
    logger = setup_logger(0)

    if world_size > 1:
        logger.info(f"ğŸš€ Spawning {world_size} DDP processes...")
        # è®¾ç½®ç¯å¢ƒå˜é‡, ä»¥ä¾¿worker_fnå¯ä»¥è®¿é—®
        os.environ["WORLD_SIZE"] = str(world_size)
        mp.spawn(worker_fn, args=(world_size, config), nprocs=world_size, join=True)
    elif world_size == 1 and torch.cuda.is_available():
        logger.info("ğŸš€ Starting single-GPU training...")
        worker_fn(0, 1, config)
    else:
        logger.error("âŒ No GPUs found or world_size is zero. Exiting.")
        exit(1)


if __name__ == "__main__":
    main()
