# ä»é›¶å¼€å§‹çš„ LLM è®­ç»ƒæŒ‡å— - CPU ç‰ˆæœ¬ (æ¦‚å¿µæ¼”ç¤º)

> [!NOTE]
> è¿™æ˜¯ä¸€ä»½**æ¦‚å¿µæ€§**çš„å…¥é—¨æŒ‡å—ï¼Œæ—¨åœ¨å¸®åŠ©åˆå­¦è€…ç†è§£è®­ç»ƒæµç¨‹çš„åŸºæœ¬åŸç†ã€‚
>
> å¦‚æœæ‚¨æƒ³å­¦ä¹ å¦‚ä½•ä½¿ç”¨æœ¬é¡¹ç›®æä¾›çš„**æ­£å¼æ¡†æ¶**ï¼ˆåŒ…å« DDPã€AMPã€å¯é…ç½®åŒ–ç­‰é«˜çº§ç‰¹æ€§ï¼‰ï¼Œè¯·é˜…è¯»å®˜æ–¹æ•™ç¨‹ï¼š
> ğŸ‘‰ [LLM æ¡†æ¶æ•™ç¨‹: ä»é›¶å¼€å§‹æ„å»ºä¸è®­ç»ƒ](../../docs/tutorial-cpu-llm.md)

## ç›®å½•

- [é˜¶æ®µä¸€: ç¯å¢ƒå‡†å¤‡ä¸åŸºç¡€çŸ¥è¯†(1-2å¤©)](#é˜¶æ®µä¸€ç¯å¢ƒå‡†å¤‡ä¸åŸºç¡€çŸ¥è¯†1-2å¤©)
- [é˜¶æ®µäºŒ: æ•°æ®å¤„ç†ä¸é¢„è®­ç»ƒæ¨¡å‹(2-3å¤©)](#é˜¶æ®µäºŒæ•°æ®å¤„ç†ä¸é¢„è®­ç»ƒæ¨¡å‹2-3å¤©)
- [é˜¶æ®µä¸‰: æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–(3-4å¤©)](#é˜¶æ®µä¸‰æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–3-4å¤©)
- [é˜¶æ®µå››: è¯„ä¼°ä¸æ‰©å±•(2-3å¤©)](#é˜¶æ®µå››è¯„ä¼°ä¸æ‰©å±•2-3å¤©)

## é˜¶æ®µä¸€: ç¯å¢ƒå‡†å¤‡ä¸åŸºç¡€çŸ¥è¯†(1-2å¤©)

### 1.1 ç¯å¢ƒé…ç½®

é¦–å…ˆåˆ›å»ºé¡¹ç›®ç›®å½•å¹¶ä½¿ç”¨ `uv` åˆå§‹åŒ–è™šæ‹Ÿç¯å¢ƒ:

```bash
mkdir llm-training
cd llm-training
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# æˆ–
.venv\Scripts\activate  # Windows
```

åˆ›å»º `pyproject.toml`:

```toml
[project]
name = "llm-training"
version = "0.1.0"
description = "LLM training project"
requires-python = ">=3.9"

dependencies = [
    "torch>=2.2.0",
    "transformers>=4.37.0",
    "datasets>=2.17.0",
    "accelerate>=0.27.0",
    "evaluate>=0.4.0",
    "scikit-learn>=1.4.0",
    "tensorboard>=2.15.0",
    "tqdm>=4.66.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.ruff]
line-length = 88
```

ä½¿ç”¨ uv å®‰è£…ä¾èµ–:

```bash
uv pip install -e .
```

### 1.2 åŸºç¡€é…ç½®æ–‡ä»¶

åˆ›å»ºé…ç½®æ–‡ä»¶ `config.py`:

```python
from dataclasses import dataclass
from typing import Optional


@dataclass
class TrainingConfig:
    # æ¨¡å‹é…ç½®
    model_name: str = "distilbert-base-uncased"
    max_length: int = 128

    # è®­ç»ƒé…ç½®
    batch_size: int = 8
    learning_rate: float = 2e-5
    num_epochs: int = 3
    warmup_steps: int = 500

    # æ•°æ®é…ç½®
    dataset_name: str = "wikitext"
    dataset_config: str = "wikitext-2-raw-v1"
    train_split: str = "train"
    eval_split: str = "test"

    # ç¡¬ä»¶é…ç½®
    device: str = "cpu"
    num_workers: int = 4

    # è¾“å‡ºé…ç½®
    output_dir: str = "outputs"
    logging_steps: int = 100
    save_steps: int = 1000


config = TrainingConfig()
```

### 1.3 å·¥å…·å‡½æ•°

åˆ›å»º `utils.py`:

```python
import os
import logging
import torch
from torch.utils.data import DataLoader
from transformers import get_linear_schedule_with_warmup


def setup_logging(name: str, level: int = logging.INFO) -> logging.Logger:
    """é…ç½®æ—¥å¿—è®°å½•å™¨"""
    logger = logging.getLogger(name)
    logger.setLevel(level)

    if not logger.handlers:
        handler = logging.StreamHandler()
        handler.setFormatter(
            logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            )
        )
        logger.addHandler(handler)

    return logger


def create_dataloaders(
    dataset,
    tokenizer,
    config,
    shuffle: bool = True
) -> DataLoader:
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""

    def collate_fn(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=config.max_length,
            return_tensors="pt"
        )

    return DataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=shuffle,
        collate_fn=collate_fn,
        num_workers=config.num_workers
    )


def get_optimizer_and_scheduler(
    model,
    config,
    num_training_steps: int
):
    """é…ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    optimizer = torch.optim.AdamW(
        model.parameters(),
        lr=config.learning_rate
    )

    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=config.warmup_steps,
        num_training_steps=num_training_steps
    )

    return optimizer, scheduler
```

## é˜¶æ®µäºŒ: æ•°æ®å¤„ç†ä¸é¢„è®­ç»ƒæ¨¡å‹(2-3å¤©)

### 2.1 æ•°æ®åŠ è½½ä¸é¢„å¤„ç†

åˆ›å»º `data.py`:

```python
from datasets import load_dataset
from transformers import AutoTokenizer


def load_and_preprocess_data(config):
    """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›†"""
    # åŠ è½½æ•°æ®é›†
    dataset = load_dataset(
        config.dataset_name,
        config.dataset_config
    )

    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(config.model_name)

    # å®šä¹‰é¢„å¤„ç†å‡½æ•°
    def preprocess_function(examples):
        return tokenizer(
            examples["text"],
            truncation=True,
            padding="max_length",
            max_length=config.max_length
        )

    # åº”ç”¨é¢„å¤„ç†
    tokenized_dataset = dataset.map(
        preprocess_function,
        batched=True,
        remove_columns=dataset["train"].column_names
    )

    return tokenized_dataset, tokenizer


def prepare_dataloaders(dataset, tokenizer, config):
    """å‡†å¤‡æ•°æ®åŠ è½½å™¨"""
    train_dataloader = create_dataloaders(
        dataset["train"],
        tokenizer,
        config,
        shuffle=True
    )

    eval_dataloader = create_dataloaders(
        dataset["test"],
        tokenizer,
        config,
        shuffle=False
    )

    return train_dataloader, eval_dataloader
```

### 2.2 æ¨¡å‹å®šä¹‰

åˆ›å»º `model.py`:

```python
from transformers import AutoModelForMaskedLM


def create_model(config):
    """åˆ›å»ºå’Œé…ç½®æ¨¡å‹"""
    model = AutoModelForMaskedLM.from_pretrained(config.model_name)
    model = model.to(config.device)
    return model
```

## é˜¶æ®µä¸‰: æ¨¡å‹è®­ç»ƒä¸ä¼˜åŒ–(3-4å¤©)

### 3.1 è®­ç»ƒå¾ªç¯

åˆ›å»º `trainer.py`:

```python
import torch
from tqdm.auto import tqdm
from torch.utils.tensorboard import SummaryWriter


class Trainer:
    def __init__(self, model, train_dataloader, eval_dataloader, config):
        self.model = model
        self.train_dataloader = train_dataloader
        self.eval_dataloader = eval_dataloader
        self.config = config

        # è®¾ç½®ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        num_training_steps = len(train_dataloader) * config.num_epochs
        self.optimizer, self.scheduler = get_optimizer_and_scheduler(
            model, config, num_training_steps
        )

        # è®¾ç½®tensorboard
        self.writer = SummaryWriter(config.output_dir)
        self.logger = setup_logging("trainer")

    def train(self):
        """è®­ç»ƒå¾ªç¯"""
        self.model.train()

        for epoch in range(self.config.num_epochs):
            self.logger.info(f"Starting epoch {epoch + 1}")

            progress_bar = tqdm(
                self.train_dataloader,
                desc=f"Epoch {epoch + 1}"
            )

            for step, batch in enumerate(progress_bar):
                # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                batch = {k: v.to(self.config.device) for k, v in batch.items()}

                # å‰å‘ä¼ æ’­
                outputs = self.model(**batch)
                loss = outputs.loss

                # åå‘ä¼ æ’­
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    max_norm=1.0
                )

                # ä¼˜åŒ–å™¨æ­¥è¿›
                self.optimizer.step()
                self.scheduler.step()
                self.optimizer.zero_grad()

                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix({"loss": loss.item()})

                # è®°å½•è®­ç»ƒæŒ‡æ ‡
                if step % self.config.logging_steps == 0:
                    self.writer.add_scalar(
                        "train/loss",
                        loss.item(),
                        step
                    )

                # ä¿å­˜æ£€æŸ¥ç‚¹
                if step % self.config.save_steps == 0:
                    self.save_checkpoint(epoch, step)

            # æ¯ä¸ªepochç»“æŸåè¯„ä¼°
            eval_loss = self.evaluate()
            self.logger.info(f"Epoch {epoch + 1} evaluation loss: {eval_loss}")

    def evaluate(self):
        """è¯„ä¼°å¾ªç¯"""
        self.model.eval()
        total_loss = 0
        num_batches = len(self.eval_dataloader)

        with torch.no_grad():
            for batch in tqdm(self.eval_dataloader, desc="Evaluating"):
                batch = {k: v.to(self.config.device) for k, v in batch.items()}
                outputs = self.model(**batch)
                total_loss += outputs.loss.item()

        avg_loss = total_loss / num_batches
        return avg_loss

    def save_checkpoint(self, epoch, step):
        """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
        checkpoint_dir = os.path.join(
            self.config.output_dir,
            f"checkpoint-{epoch}-{step}"
        )
        os.makedirs(checkpoint_dir, exist_ok=True)

        # ä¿å­˜æ¨¡å‹
        self.model.save_pretrained(checkpoint_dir)

        # ä¿å­˜è®­ç»ƒçŠ¶æ€
        torch.save({
            'epoch': epoch,
            'step': step,
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
        }, os.path.join(checkpoint_dir, "training_state.pt"))
```

### 3.2 ä¸»è®­ç»ƒè„šæœ¬

åˆ›å»º `train.py`:

```python
from config import config
from data import load_and_preprocess_data, prepare_dataloaders
from model import create_model
from trainer import Trainer


def main():
    # åŠ è½½æ•°æ®
    dataset, tokenizer = load_and_preprocess_data(config)
    train_dataloader, eval_dataloader = prepare_dataloaders(
        dataset, tokenizer, config
    )

    # åˆ›å»ºæ¨¡å‹
    model = create_model(config)

    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = Trainer(
        model,
        train_dataloader,
        eval_dataloader,
        config
    )

    # å¼€å§‹è®­ç»ƒ
    trainer.train()


if __name__ == "__main__":
    main()
```

## é˜¶æ®µå››: è¯„ä¼°ä¸æ‰©å±•(2-3å¤©)

### 4.1 æ¨¡å‹è¯„ä¼°

åˆ›å»º `evaluate.py`:

```python
import torch
from sklearn.metrics import accuracy_score
import numpy as np


def evaluate_model(model, eval_dataloader, config):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    model.eval()
    predictions = []
    references = []

    with torch.no_grad():
        for batch in eval_dataloader:
            batch = {k: v.to(config.device) for k, v in batch.items()}
            outputs = model(**batch)

            # è·å–é¢„æµ‹ç»“æœ
            pred = torch.argmax(outputs.logits, dim=-1)
            predictions.extend(pred.cpu().numpy())

            # è·å–çœŸå®æ ‡ç­¾
            refs = batch["labels"]
            references.extend(refs.cpu().numpy())

    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = accuracy_score(references, predictions)

    # è®¡ç®—å›°æƒ‘åº¦
    perplexity = torch.exp(torch.tensor(outputs.loss)).item()

    return {
        "accuracy": accuracy,
        "perplexity": perplexity
    }
```

### 4.2 æ€§èƒ½ä¼˜åŒ–å»ºè®®

å¯¹äºCPUè®­ç»ƒ, ä»¥ä¸‹æ˜¯ä¸€äº›ä¼˜åŒ–å»ºè®®:

1. æ•°æ®åŠ è½½ä¼˜åŒ–:
    - ä½¿ç”¨ `num_workers` è¿›è¡Œå¹¶è¡Œæ•°æ®åŠ è½½
    - é€‚å½“è°ƒæ•´ `batch_size`
    - ä½¿ç”¨ `pin_memory=True` åŠ é€Ÿæ•°æ®ä¼ è¾“

2. æ¨¡å‹ä¼˜åŒ–:
    - ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ(è™½ç„¶åœ¨CPUä¸Šæ”¶ç›Šè¾ƒå°)
    - ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯å‡å°‘å†…å­˜å ç”¨
    - é€‰æ‹©è¾ƒå°çš„æ¨¡å‹æ¶æ„(å¦‚DistilBERT)

3. è®­ç»ƒç­–ç•¥ä¼˜åŒ–:
    - ä½¿ç”¨è¾ƒå°çš„åºåˆ—é•¿åº¦
    - å®æ–½æ—©åœæœºåˆ¶
    - ä½¿ç”¨å­¦ä¹ ç‡é¢„çƒ­å’Œè¡°å‡

### 4.3 æ‰©å±•å»ºè®®

1. ç¡¬ä»¶å‡çº§è·¯çº¿:
    - é¦–å…ˆå‡çº§åˆ°æ›´å¤§å®¹é‡çš„å†…å­˜(128GB)
    - æ·»åŠ å…¥é—¨çº§GPU(å¦‚NVIDIA RTX 3060)
    - è€ƒè™‘ä½¿ç”¨äº‘æœåŠ¡è¿›è¡Œå¤§è§„æ¨¡è®­ç»ƒ

2. åˆ†å¸ƒå¼è®­ç»ƒå‡†å¤‡:
    - å­¦ä¹ PyTorch DDP(DistributedDataParallel)
    - äº†è§£Hugging Face Accelerateåº“
    - ç ”ç©¶æ¨¡å‹å¹¶è¡Œå’Œæ•°æ®å¹¶è¡Œç­–ç•¥

## ä½¿ç”¨è¯´æ˜

1. å…‹éš†ä»£ç ä»“åº“:

```bash
git clone <repository-url>
cd llm-training
```

1. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ:

```bash
python -m venv .venv
source .venv/bin/activate  # Linux/Mac
# æˆ–
.venv\Scripts\activate  # Windows
```

1. å®‰è£…ä¾èµ–:

```bash
uv pip install -e .
```

1. è¿è¡Œè®­ç»ƒ:

```bash
python best_minimal_train.py
```

## è®­ç»ƒè¿‡ç¨‹ä¼˜åŒ–

### 1. å†…å­˜ä¼˜åŒ–

åˆ›å»º `memory_utils.py`:

```python
import gc
import torch
from typing import Optional


def optimize_memory_usage(
    batch_size: int,
    sequence_length: int,
    vocab_size: int,
    hidden_size: int,
    available_memory: Optional[float] = None
) -> dict:
    """
    è®¡ç®—å¹¶ä¼˜åŒ–å†…å­˜ä½¿ç”¨
    """
    # ä¼°ç®—å•ä¸ªæ ·æœ¬çš„å†…å­˜å ç”¨
    sample_size = sequence_length * hidden_size * 4  # float32 = 4 bytes
    batch_memory = sample_size * batch_size

    # ä¼°ç®—æ¨¡å‹å‚æ•°çš„å†…å­˜å ç”¨
    model_memory = (vocab_size * hidden_size + hidden_size * hidden_size) * 4

    # ä¼°ç®—ä¼˜åŒ–å™¨çŠ¶æ€çš„å†…å­˜å ç”¨
    optimizer_memory = model_memory * 2  # AdamW éœ€è¦å­˜å‚¨åŠ¨é‡å’Œæ–¹å·®

    total_memory = batch_memory + model_memory + optimizer_memory

    # è¿”å›ä¼˜åŒ–å»ºè®®
    return {
        "recommended_batch_size": min(batch_size, max(1,
                                                      batch_size * available_memory // total_memory)) if available_memory else batch_size,
        "estimated_memory_usage": total_memory / (1024 ** 3),  # Convert to GB
        "can_use_gradient_accumulation": total_memory > (available_memory if available_memory else float('inf'))
    }


def clear_memory():
    """
    æ¸…ç†æœªä½¿ç”¨çš„å†…å­˜
    """
    gc.collect()
    torch.cuda.empty_cache() if torch.cuda.is_available() else None
```

### 2. æ¢¯åº¦ç´¯ç§¯å®ç°

æ›´æ–° `trainer.py` æ·»åŠ æ¢¯åº¦ç´¯ç§¯æ”¯æŒ:

```python
class GradientAccumulationTrainer(Trainer):
    def __init__(self, *args, accumulation_steps: int = 4, **kwargs):
        super().__init__(*args, **kwargs)
        self.accumulation_steps = accumulation_steps

    def train(self):
        self.model.train()

        for epoch in range(self.config.num_epochs):
            self.logger.info(f"Starting epoch {epoch + 1}")

            progress_bar = tqdm(self.train_dataloader, desc=f"Epoch {epoch + 1}")

            for step, batch in enumerate(progress_bar):
                # å°†æ•°æ®ç§»åŠ¨åˆ°è®¾å¤‡
                batch = {k: v.to(self.config.device) for k, v in batch.items()}

                # å‰å‘ä¼ æ’­
                outputs = self.model(**batch)
                loss = outputs.loss / self.accumulation_steps  # ç¼©æ”¾æŸå¤±

                # åå‘ä¼ æ’­
                loss.backward()

                # æ¯ accumulation_steps æ­¥è¿›è¡Œä¸€æ¬¡ä¼˜åŒ–å™¨æ›´æ–°
                if (step + 1) % self.accumulation_steps == 0:
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        max_norm=1.0
                    )
                    self.optimizer.step()
                    self.scheduler.step()
                    self.optimizer.zero_grad()

                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix({"loss": loss.item() * self.accumulation_steps})

                # è®°å½•è®­ç»ƒæŒ‡æ ‡
                if step % self.config.logging_steps == 0:
                    self.writer.add_scalar(
                        "train/loss",
                        loss.item() * self.accumulation_steps,
                        step
                    )
```

### 3. æå‰åœæ­¢æœºåˆ¶

åˆ›å»º `early_stopping.py`:

```python
class EarlyStopping:
    def __init__(
        self,
        patience: int = 3,
        min_delta: float = 0.0,
        mode: str = "min"
    ):
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, current_loss: float) -> bool:
        if self.best_loss is None:
            self.best_loss = current_loss
            return False

        if self.mode == "min":
            if current_loss < self.best_loss - self.min_delta:
                self.best_loss = current_loss
                self.counter = 0
            else:
                self.counter += 1
        else:
            if current_loss > self.best_loss + self.min_delta:
                self.best_loss = current_loss
                self.counter = 0
            else:
                self.counter += 1

        if self.counter >= self.patience:
            self.early_stop = True
            return True

        return False
```

## é«˜çº§åŠŸèƒ½æ‰©å±•

### 1. è‡ªå®šä¹‰æ•°æ®é›†æ”¯æŒ

åˆ›å»º `custom_dataset.py`:

```python
from torch.utils.data import Dataset
from typing import List, Dict, Optional
import json


class CustomTextDataset(Dataset):
    def __init__(
        self,
        texts: List[str],
        labels: Optional[List[int]] = None,
        tokenizer=None,
        max_length: int = 128
    ):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]

        # åˆ†è¯
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt"
        )

        # ç§»é™¤æ‰¹æ¬¡ç»´åº¦
        item = {
            key: val.squeeze(0) for key, val in encoding.items()
        }

        # æ·»åŠ æ ‡ç­¾(å¦‚æœæœ‰)
        if self.labels is not None:
            item["labels"] = self.labels[idx]

        return item

    @classmethod
    def from_json(cls, json_path: str, **kwargs):
        """ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®é›†"""
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)

        texts = [item["text"] for item in data]
        labels = [item.get("label") for item in data] if "label" in data[0] else None

        return cls(texts, labels, **kwargs)
```

### 2. å®éªŒè·Ÿè¸ª

åˆ›å»º `experiment_tracking.py`:

```python
import json
import os
from datetime import datetime
from typing import Dict, Any


class ExperimentTracker:
    def __init__(self, base_dir: str = "experiments"):
        self.base_dir = base_dir
        self.experiment_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.experiment_dir = os.path.join(base_dir, self.experiment_id)
        os.makedirs(self.experiment_dir, exist_ok=True)

        self.metrics = []
        self.config = None

    def log_config(self, config: Dict[str, Any]):
        """è®°å½•å®éªŒé…ç½®"""
        self.config = config
        with open(os.path.join(self.experiment_dir, "config.json"), "w") as f:
            json.dump(config, f, indent=2)

    def log_metrics(self, metrics: Dict[str, float], step: int):
        """è®°å½•è®­ç»ƒæŒ‡æ ‡"""
        metrics["step"] = step
        self.metrics.append(metrics)

        with open(os.path.join(self.experiment_dir, "metrics.json"), "w") as f:
            json.dump(self.metrics, f, indent=2)

    def save_artifact(self, name: str, content: Any):
        """ä¿å­˜å®éªŒäº§ç‰©"""
        artifact_path = os.path.join(self.experiment_dir, "artifacts", name)
        os.makedirs(os.path.dirname(artifact_path), exist_ok=True)

        if isinstance(content, (dict, list)):
            with open(artifact_path, "w") as f:
                json.dump(content, f, indent=2)
        else:
            with open(artifact_path, "w") as f:
                f.write(str(content))
```

## è¿›é˜¶å­¦ä¹ è·¯çº¿å»ºè®®

1. ç†è®ºæ·±åŒ–:
    - å­¦ä¹  Transformer æ¶æ„è¯¦ç»†åŸç†
    - ç ”ç©¶ä¸åŒçš„æ³¨æ„åŠ›æœºåˆ¶
    - äº†è§£å„ç§é¢„è®­ç»ƒä»»åŠ¡çš„è®¾è®¡

2. å®è·µæå‡:
    - å°è¯•å®ç°ç®€å•çš„ Transformer æ¨¡å—
    - æ¢ç´¢ä¸åŒçš„ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦ç­–ç•¥
    - å®éªŒå„ç§æ­£åˆ™åŒ–æŠ€æœ¯

3. å·¥ç¨‹æŠ€èƒ½:
    - å­¦ä¹ åˆ†å¸ƒå¼è®­ç»ƒæ¡†æ¶(å¦‚ PyTorch Lightning)
    - æŒæ¡æ¨¡å‹é‡åŒ–å’Œå‹ç¼©æŠ€æœ¯
    - ç ”ç©¶æ¨¡å‹éƒ¨ç½²å’ŒæœåŠ¡åŒ–æ–¹æ¡ˆ

## å¸¸è§é—®é¢˜è§£ç­”

1. å†…å­˜ä¸è¶³é—®é¢˜:
    - å‡å°æ‰¹æ¬¡å¤§å°
    - ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
    - å®æ–½æ¨¡å‹æ£€æŸ¥ç‚¹æœºåˆ¶
    - ä½¿ç”¨è¾ƒå°çš„åºåˆ—é•¿åº¦

2. è®­ç»ƒé€Ÿåº¦æ…¢:
    - ä½¿ç”¨è¾ƒå°çš„æ¨¡å‹æ¶æ„
    - ä¼˜åŒ–æ•°æ®åŠ è½½æµç¨‹
    - å®æ–½å¤šè¿›ç¨‹æ•°æ®é¢„å¤„ç†
    - è€ƒè™‘ä½¿ç”¨é¢„è®¡ç®—çš„ç‰¹å¾

3. æ¨¡å‹æ•ˆæœä¸ä½³:
    - æ£€æŸ¥æ•°æ®è´¨é‡å’Œé¢„å¤„ç†æµç¨‹
    - è°ƒæ•´å­¦ä¹ ç‡å’Œä¼˜åŒ–å™¨å‚æ•°
    - å¢åŠ è®­ç»ƒè½®æ¬¡
    - ä½¿ç”¨äº¤å‰éªŒè¯é€‰æ‹©æœ€ä½³å‚æ•°

## é¡¹ç›®ç»“æ„å’Œä»£ç ç»„ç»‡

```
llm-training/
â”œâ”€â”€ .venv/                    # è™šæ‹Ÿç¯å¢ƒç›®å½•
â”œâ”€â”€ data/                     # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ raw/                 # åŸå§‹æ•°æ®
â”‚   â””â”€â”€ processed/           # é¢„å¤„ç†åçš„æ•°æ®
â”œâ”€â”€ src/                     # æºä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py           # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ data.py             # æ•°æ®å¤„ç†
â”‚   â”œâ”€â”€ model.py            # æ¨¡å‹å®šä¹‰
â”‚   â”œâ”€â”€ trainer.py          # è®­ç»ƒå™¨
â”‚   â”œâ”€â”€ evaluate.py         # è¯„ä¼°è„šæœ¬
â”‚   â”œâ”€â”€ utils/              # å·¥å…·å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ memory_utils.py
â”‚   â”‚   â””â”€â”€ logging_utils.py
â”‚   â””â”€â”€ experiments/        # å®éªŒè·Ÿè¸ª
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ tracking.py
â”œâ”€â”€ notebooks/               # Jupyter notebooks
â”‚   â”œâ”€â”€ 1_data_exploration.ipynb
â”‚   â””â”€â”€ 2_model_analysis.ipynb
â”œâ”€â”€ tests/                   # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_data.py
â”‚   â””â”€â”€ test_model.py
â”œâ”€â”€ outputs/                 # æ¨¡å‹è¾“å‡º
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ examples/                # ç¤ºä¾‹ä»£ç 
â”‚   â””â”€â”€ custom_dataset_example.py
â”œâ”€â”€ scripts/                 # å®ç”¨è„šæœ¬
â”‚   â”œâ”€â”€ prepare_data.sh
â”‚   â””â”€â”€ train_model.sh
â”œâ”€â”€ pyproject.toml          # é¡¹ç›®é…ç½®
â”œâ”€â”€ README.md               # é¡¹ç›®è¯´æ˜
â””â”€â”€ requirements.txt        # ä¾èµ–æ¸…å•
```

## é”™è¯¯å¤„ç†å’Œè°ƒè¯•æŒ‡å—

### 1. å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

1. å†…å­˜ç›¸å…³é”™è¯¯:

    ```python
    # é”™è¯¯: RuntimeError: out of memory
    # è§£å†³æ–¹æ¡ˆ: å®ç°æ¸è¿›å¼æ•°æ®åŠ è½½
    from typing import Iterator, List


    def batch_generator(data: List, batch_size: int) -> Iterator:
        for i in range(0, len(data), batch_size):
            yield data[i:i + batch_size]


    # ä½¿ç”¨ç¤ºä¾‹
    for batch in batch_generator(large_dataset, batch_size=32):
        process_batch(batch)
    ```

2. æ•°æ®åŠ è½½é”™è¯¯:

    ```python
    # é”™è¯¯: FileNotFoundError: [Errno 2] No such file or directory
    # è§£å†³æ–¹æ¡ˆ: æ·»åŠ è·¯å¾„æ£€æŸ¥
    def safe_load_data(file_path: str):
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        try:
            # å°è¯•ä¸åŒçš„ç¼–ç 
            for encoding in ['utf-8', 'gbk', 'latin1']:
                try:
                    with open(file_path, 'r', encoding=encoding) as f:
                        return f.read()
                except UnicodeDecodeError:
                    continue
            raise UnicodeDecodeError("æ— æ³•è¯†åˆ«æ–‡ä»¶ç¼–ç ")
        except Exception as e:
            raise Exception(f"åŠ è½½æ•°æ®å¤±è´¥: {str(e)}")
    ```

3. æ¨¡å‹è®­ç»ƒé”™è¯¯:

    ```python
    # é”™è¯¯: Loss is NaN
    # è§£å†³æ–¹æ¡ˆ: æ·»åŠ æ¢¯åº¦è£å‰ªå’ŒæŸå¤±æ£€æŸ¥
    def check_loss(loss: torch.Tensor) -> bool:
        if torch.isnan(loss) or torch.isinf(loss):
            logger.error(f"æ£€æµ‹åˆ°æ— æ•ˆæŸå¤±å€¼: {loss.item()}")
            return False
        return True

    # è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
    loss = outputs.loss
    if not check_loss(loss):
        logger.info("è·³è¿‡å½“å‰æ‰¹æ¬¡")
        continue
    ```

### 2. è°ƒè¯•æŠ€å·§

1. ä½¿ç”¨æ—¥å¿—è®°å½•å…³é”®ä¿¡æ¯:

    ```python
    import logging

    def setup_debug_logging():
        logging.basicConfig(
            level=logging.DEBUG,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('debug.log'),
                logging.StreamHandler()
            ]
        )

    # åœ¨å…³é”®ä½ç½®æ·»åŠ æ—¥å¿—
    logging.debug(f"æ•°æ®å½¢çŠ¶: {batch.shape}")
    logging.debug(f"å½“å‰å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']}")
    ```

2. æ·»åŠ æ–­è¨€æ£€æŸ¥:

    ```python
    def validate_inputs(inputs: dict):
        """éªŒè¯æ¨¡å‹è¾“å…¥"""
        assert 'input_ids' in inputs, "ç¼ºå°‘ input_ids"
        assert 'attention_mask' in inputs, "ç¼ºå°‘ attention_mask"
        assert inputs['input_ids'].dim() == 2, f"input_ids ç»´åº¦é”™è¯¯: {inputs['input_ids'].dim()}"
        return True
    ```

3. æ€§èƒ½åˆ†æå·¥å…·:

    ```python
    import cProfile
    import pstats


    def profile_function(func):
        """å‡½æ•°æ€§èƒ½åˆ†æè£…é¥°å™¨"""

        def wrapper(*args, **kwargs):
            profile = cProfile.Profile()
            try:
                return profile.runcall(func, *args, **kwargs)
            finally:
                ps = pstats.Stats(profile)
                ps.sort_stats('cumulative')
                ps.print_stats(20)  # æ‰“å°å‰20è¡Œç»Ÿè®¡ä¿¡æ¯

        return wrapper


    @profile_function
    def train_epoch(model, dataloader):
        # è®­ç»ƒä»£ç 
        pass
    ```

## å®è·µä»»åŠ¡ç¤ºä¾‹

### ä»»åŠ¡1: æ–‡æœ¬åˆ†ç±»

åˆ›å»ºä¸€ä¸ªç®€å•çš„æ–‡æœ¬åˆ†ç±»æ¨¡å‹:

```python
from transformers import DistilBertForSequenceClassification


def create_classification_model(num_labels: int = 2):
    model = DistilBertForSequenceClassification.from_pretrained(
        "distilbert-base-uncased",
        num_labels=num_labels
    )
    return model


# ä½¿ç”¨ç¤ºä¾‹
model = create_classification_model()
outputs = model(**batch)
loss = outputs.loss
logits = outputs.logits
```

### ä»»åŠ¡2: æ–‡æœ¬ç”Ÿæˆ

å®ç°ä¸€ä¸ªç®€å•çš„æ–‡æœ¬ç”Ÿæˆä»»åŠ¡:

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer


def generate_text(
    prompt: str,
    max_length: int = 50,
    num_return_sequences: int = 1
):
    tokenizer = GPT2Tokenizer.from_pretrained("distilgpt2")
    model = GPT2LMHeadModel.from_pretrained("distilgpt2")

    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(
        **inputs,
        max_length=max_length,
        num_return_sequences=num_return_sequences,
        no_repeat_ngram_size=2,
        temperature=0.7
    )

    return tokenizer.batch_decode(outputs, skip_special_tokens=True)


# ä½¿ç”¨ç¤ºä¾‹
generated_texts = generate_text("Once upon a time")
```

### ä»»åŠ¡3: æ¨¡å‹å¾®è°ƒ

```python
def fine_tune_model(
    model,
    train_dataset,
    eval_dataset,
    output_dir: str,
    epochs: int = 3
):
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=epochs,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir='./logs',
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset
    )

    trainer.train()
    return trainer


# ä½¿ç”¨ç¤ºä¾‹
trainer = fine_tune_model(model, train_dataset, eval_dataset, "./output")
```

## èµ„æºæ¨è

1. å­¦ä¹ èµ„æº:
    - Hugging Face è¯¾ç¨‹
    - "Attention is All You Need" è®ºæ–‡
    - The Annotated Transformer
    - PyTorch å®˜æ–¹æ•™ç¨‹

2. å¼€æºé¡¹ç›®:
    - transformers åº“
    - datasets åº“
    - accelerate åº“
    - PyTorch Lightning

3. ç¤¾åŒºèµ„æº:
    - Hugging Face è®ºå›
    - PyTorch è®ºå›
    - Papers with Code
    - arXiv ML ç‰ˆå—

è¿™ä¸ªæ•™ç¨‹æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„æ¡†æ¶, å¸®åŠ©ä½ åœ¨æœ¬åœ° CPU ç¯å¢ƒä¸‹å¼€å§‹ LLM è®­ç»ƒ. å»ºè®®æŒ‰ç…§é˜¶æ®µå¾ªåºæ¸è¿›åœ°å­¦ä¹ , ç¡®ä¿æ¯ä¸ªæ¦‚å¿µéƒ½æŒæ¡åå†è¿›å…¥ä¸‹ä¸€é˜¶æ®µ. åŒæ—¶, å¯ä»¥æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´å­¦ä¹ è®¡åˆ’å’Œè¿›åº¦.
