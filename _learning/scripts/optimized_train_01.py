"""
ä¼˜åŒ–åçš„PyTorchåˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬
ç»“æ„æ›´æ¸…æ™°ï¼Œæ¨¡å—åŒ–ç¨‹åº¦æ›´é«˜ï¼Œæ˜“äºç»´æŠ¤å’Œæ‰©å±•
"""

import argparse
import logging
import os
import time
from dataclasses import dataclass
from pathlib import Path

import torch
import torch.distributed as dist
import torch.multiprocessing as mp
import torch.nn as nn
import torch.optim as optim
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler, TensorDataset

# ============================================================================
# é…ç½®ç®¡ç†
# ============================================================================


@dataclass
class ModelConfig:
    """æ¨¡å‹é…ç½®"""

    hidden_size: int = 512
    ffn_hidden_size: int = 2048


@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®"""

    batch_size: int = 128
    epochs: int = 10
    lr: float = 1e-3
    weight_decay: float = 0.01
    num_samples: int = 20000


@dataclass
class DistributedConfig:
    """åˆ†å¸ƒå¼é…ç½®"""

    master_addr: str = "127.0.0.1"
    master_port: str = "12355"
    num_nodes: int = 1
    gpus_per_node: int = torch.cuda.device_count()
    node_rank: int = 0


@dataclass
class OptimizationConfig:
    """æ€§èƒ½ä¼˜åŒ–é…ç½®"""

    use_compile: bool = True
    use_amp: bool = True
    num_workers: int = 4
    pin_memory: bool = True


@dataclass
class CheckpointConfig:
    """æ£€æŸ¥ç‚¹é…ç½®"""

    checkpoint_dir: str = "checkpoints"
    resume_from_checkpoint: str | None = None
    save_interval: int = 1  # æ¯éš”å¤šå°‘ä¸ªepochä¿å­˜ä¸€æ¬¡


@dataclass
class Config:
    """ä¸»é…ç½®ç±»ï¼Œç»„åˆæ‰€æœ‰é…ç½®"""

    model: ModelConfig
    training: TrainingConfig
    distributed: DistributedConfig
    optimization: OptimizationConfig
    checkpoint: CheckpointConfig

    @classmethod
    def from_args_and_env(cls) -> "Config":
        """ä»å‘½ä»¤è¡Œå‚æ•°å’Œç¯å¢ƒå˜é‡åˆ›å»ºé…ç½®"""
        parser = argparse.ArgumentParser(description="PyTorch DDP Training Script")

        # è®­ç»ƒå‚æ•°
        parser.add_argument("--epochs", type=int, help="Number of training epochs")
        parser.add_argument("--batch-size", type=int, help="Batch size per GPU")
        parser.add_argument("--lr", type=float, help="Learning rate")
        parser.add_argument("--hidden-size", type=int, help="Hidden size of the model")

        # ä¼˜åŒ–å‚æ•°
        parser.add_argument("--no-compile", action="store_true", help="Disable torch.compile")
        parser.add_argument("--no-amp", action="store_true", help="Disable AMP")

        # æ£€æŸ¥ç‚¹å‚æ•°
        parser.add_argument("--resume-from", type=str, help="Path to checkpoint to resume from")
        parser.add_argument("--checkpoint-dir", type=str, help="Directory to save checkpoints")

        args = parser.parse_args()

        # åˆ›å»ºé»˜è®¤é…ç½®
        config = cls(
            model=ModelConfig(),
            training=TrainingConfig(),
            distributed=DistributedConfig(),
            optimization=OptimizationConfig(),
            checkpoint=CheckpointConfig(),
        )

        # ä»ç¯å¢ƒå˜é‡æ›´æ–°åˆ†å¸ƒå¼é…ç½®
        config.distributed.node_rank = int(os.environ.get("NODE_RANK", 0))
        config.distributed.num_nodes = int(os.environ.get("NUM_NODES", 1))
        config.distributed.master_addr = os.environ.get("MASTER_ADDR", "127.0.0.1")
        config.distributed.master_port = os.environ.get("MASTER_PORT", "12355")
        config.distributed.gpus_per_node = int(os.environ.get("GPUS_PER_NODE", torch.cuda.device_count()))

        # ä»å‘½ä»¤è¡Œå‚æ•°æ›´æ–°é…ç½®
        if args.epochs is not None:
            config.training.epochs = args.epochs
        if args.batch_size is not None:
            config.training.batch_size = args.batch_size
        if args.lr is not None:
            config.training.lr = args.lr
        if args.hidden_size is not None:
            config.model.hidden_size = args.hidden_size
            config.model.ffn_hidden_size = args.hidden_size * 4
        if args.no_compile:
            config.optimization.use_compile = False
        if args.no_amp:
            config.optimization.use_amp = False
        if args.resume_from:
            config.checkpoint.resume_from_checkpoint = args.resume_from
        if args.checkpoint_dir:
            config.checkpoint.checkpoint_dir = args.checkpoint_dir

        return config


# ============================================================================
# æ—¥å¿—ç®¡ç†
# ============================================================================


class Logger:
    """æ—¥å¿—ç®¡ç†å™¨"""

    def __init__(self, rank: int):
        self.rank = rank
        self.logger = logging.getLogger(__name__)
        self._setup_logging()

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—"""
        self.logger.setLevel(logging.INFO)
        formatter = logging.Formatter(f"[%(asctime)s] [%(levelname)s] [Rank {self.rank}] %(message)s")

        if self.rank == 0:
            handler = logging.StreamHandler()
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
        else:
            self.logger.addHandler(logging.NullHandler())

    def info(self, msg: str):
        self.logger.info(msg)

    def warning(self, msg: str):
        self.logger.warning(msg)

    def error(self, msg: str):
        self.logger.error(msg)

    def debug(self, msg: str):
        self.logger.debug(msg)

    def exception(self, msg: str):
        self.logger.exception(msg)


# ============================================================================
# æ¨¡å‹å®šä¹‰
# ============================================================================


class SimpleMLP(nn.Module):
    """ç®€å•çš„å¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹"""

    def __init__(self, config: ModelConfig):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(config.hidden_size, config.ffn_hidden_size),
            nn.ReLU(),
            nn.Linear(config.ffn_hidden_size, config.hidden_size),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        return self.net(x)


# ============================================================================
# åˆ†å¸ƒå¼è®­ç»ƒç®¡ç†
# ============================================================================


class DistributedManager:
    """åˆ†å¸ƒå¼è®­ç»ƒç®¡ç†å™¨"""

    def __init__(self, config: DistributedConfig):
        self.config = config

    def setup(self, rank: int, world_size: int):
        """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
        os.environ["MASTER_ADDR"] = self.config.master_addr
        os.environ["MASTER_PORT"] = self.config.master_port

        # ä½¿ç”¨NCCLåç«¯è¿›è¡ŒGPUé€šä¿¡
        dist.init_process_group(backend="nccl", rank=rank, world_size=world_size)
        torch.cuda.set_device(rank % torch.cuda.device_count())

    @staticmethod
    def cleanup():
        """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
        if dist.is_initialized():
            dist.destroy_process_group()

    def get_world_size(self) -> int:
        """è·å–æ€»çš„è¿›ç¨‹æ•°"""
        return self.config.num_nodes * self.config.gpus_per_node


# ============================================================================
# æ•°æ®ç®¡ç†
# ============================================================================


class DataManager:
    """æ•°æ®ç®¡ç†å™¨"""

    def __init__(self, config: Config, rank: int, world_size: int):
        self.config = config
        self.rank = rank
        self.world_size = world_size

    def create_dataloader(self) -> tuple[DataLoader, DistributedSampler]:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        # åˆ›å»ºåˆæˆæ•°æ®é›†
        x = torch.randn(self.config.training.num_samples, self.config.model.hidden_size)
        y = torch.randn(self.config.training.num_samples, self.config.model.hidden_size)
        dataset = TensorDataset(x, y)

        # åˆ›å»ºåˆ†å¸ƒå¼é‡‡æ ·å™¨
        sampler = DistributedSampler(dataset, num_replicas=self.world_size, rank=self.rank, shuffle=True)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        dataloader = DataLoader(
            dataset,
            batch_size=self.config.training.batch_size,
            sampler=sampler,
            num_workers=self.config.optimization.num_workers,
            pin_memory=self.config.optimization.pin_memory,
        )

        return dataloader, sampler


# ============================================================================
# æ£€æŸ¥ç‚¹ç®¡ç†
# ============================================================================


class CheckpointManager:
    """æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""

    def __init__(self, config: CheckpointConfig, rank: int, logger: Logger):
        self.config = config
        self.rank = rank
        self.logger = logger

    def save_checkpoint(
        self, epoch: int, model: DDP, optimizer: optim.Optimizer, scaler: torch.amp.GradScaler, loss: float
    ):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        if self.rank != 0:
            return

        Path(self.config.checkpoint_dir).mkdir(parents=True, exist_ok=True)
        checkpoint_path = os.path.join(self.config.checkpoint_dir, f"epoch_{epoch}.pt")

        checkpoint = {
            "epoch": epoch,
            "loss": loss,
            "model_state": model.module.state_dict(),
            "optimizer_state": optimizer.state_dict(),
            "scaler_state": scaler.state_dict(),
        }

        torch.save(checkpoint, checkpoint_path)
        self.logger.debug(f"Checkpoint saved to {checkpoint_path}")

    def load_checkpoint(
        self, model: nn.Module, optimizer: optim.Optimizer, scaler: torch.amp.GradScaler, rank: int
    ) -> int:
        """åŠ è½½æ£€æŸ¥ç‚¹ï¼Œè¿”å›èµ·å§‹epoch"""
        if not self.config.resume_from_checkpoint:
            return 0

        ckp_path = self.config.resume_from_checkpoint
        if not os.path.exists(ckp_path):
            self.logger.warning(f"Checkpoint file not found: {ckp_path}. Starting from scratch.")
            return 0

        map_location = {"cuda:0": f"cuda:{rank}"}
        checkpoint = torch.load(ckp_path, map_location=map_location)

        model.load_state_dict(checkpoint["model_state"])
        optimizer.load_state_dict(checkpoint["optimizer_state"])
        scaler.load_state_dict(checkpoint["scaler_state"])

        start_epoch = checkpoint["epoch"] + 1
        self.logger.info(f"Resumed training from epoch {start_epoch} using checkpoint {ckp_path}")

        return start_epoch


# ============================================================================
# è®­ç»ƒå™¨
# ============================================================================


class Trainer:
    """è®­ç»ƒå™¨ä¸»ç±»"""

    def __init__(self, config: Config, rank: int, world_size: int):
        self.config = config
        self.rank = rank
        self.world_size = world_size
        self.device = torch.device(f"cuda:{rank % torch.cuda.device_count()}")

        # åˆå§‹åŒ–å„ä¸ªç®¡ç†å™¨
        self.logger = Logger(rank)
        self.checkpoint_manager = CheckpointManager(config.checkpoint, rank, self.logger)
        self.data_manager = DataManager(config, rank, world_size)

        # åˆå§‹åŒ–æ¨¡å‹å’Œè®­ç»ƒç»„ä»¶
        self._setup_model()
        self._setup_training_components()
        self._setup_data()

    def _setup_model(self):
        """è®¾ç½®æ¨¡å‹"""
        # åˆ›å»ºæ¨¡å‹
        model = SimpleMLP(self.config.model)

        # åŠ è½½æ£€æŸ¥ç‚¹ï¼ˆåœ¨DDPåŒ…è£…ä¹‹å‰ï¼‰
        self.start_epoch = self.checkpoint_manager.load_checkpoint(model, None, None, self.rank)

        # ç§»åŠ¨åˆ°è®¾å¤‡
        model = model.to(self.device)

        # ç¼–è¯‘æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.optimization.use_compile:
            self.logger.info("ğŸš€ Compiling model with torch.compile...")
            model = torch.compile(model, mode="reduce-overhead")

        # åŒ…è£…ä¸ºDDP
        self.model = DDP(model, device_ids=[self.device.index])

    def _setup_training_components(self):
        """è®¾ç½®è®­ç»ƒç»„ä»¶"""
        # ä¼˜åŒ–å™¨
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=self.config.training.lr,
            weight_decay=self.config.training.weight_decay,
            fused=torch.cuda.is_available(),
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=self.config.training.epochs)

        # æ··åˆç²¾åº¦ç¼©æ”¾å™¨
        self.scaler = torch.amp.GradScaler("cuda", enabled=self.config.optimization.use_amp)

        # æŸå¤±å‡½æ•°
        self.criterion = nn.MSELoss()

        # å¦‚æœæœ‰æ£€æŸ¥ç‚¹ï¼Œé‡æ–°åŠ è½½ä¼˜åŒ–å™¨å’Œç¼©æ”¾å™¨çŠ¶æ€
        if self.config.checkpoint.resume_from_checkpoint:
            self.start_epoch = self.checkpoint_manager.load_checkpoint(
                self.model.module, self.optimizer, self.scaler, self.rank
            )

    def _setup_data(self):
        """è®¾ç½®æ•°æ®"""
        self.dataloader, self.sampler = self.data_manager.create_dataloader()

    def _run_epoch(self, epoch: int) -> float:
        """è¿è¡Œä¸€ä¸ªepoch"""
        self.sampler.set_epoch(epoch)
        self.model.train()
        total_loss = 0.0

        for data, target in self.dataloader:
            data = data.to(self.device, non_blocking=True)
            target = target.to(self.device, non_blocking=True)

            # æ¸…é›¶æ¢¯åº¦
            self.optimizer.zero_grad(set_to_none=True)

            # å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨æ··åˆç²¾åº¦ï¼‰
            with torch.amp.autocast("cuda", enabled=self.config.optimization.use_amp):
                output = self.model(data)
                loss = self.criterion(output, target)

            # åå‘ä¼ æ’­
            self.scaler.scale(loss).backward()
            self.scaler.step(self.optimizer)
            self.scaler.update()

            total_loss += loss.item()

        return total_loss / len(self.dataloader)

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        self.logger.info("ğŸ‰ Starting training...")
        start_time = time.time()

        for epoch in range(self.start_epoch, self.config.training.epochs):
            # è®­ç»ƒä¸€ä¸ªepoch
            avg_loss = self._run_epoch(epoch)

            # æ›´æ–°å­¦ä¹ ç‡
            self.scheduler.step()

            # åŒæ­¥æ‰€æœ‰è¿›ç¨‹
            dist.barrier()

            # æ—¥å¿—è®°å½•å’Œæ£€æŸ¥ç‚¹ä¿å­˜ï¼ˆä»…rank 0ï¼‰
            if self.rank == 0:
                elapsed = time.time() - start_time
                lr = self.scheduler.get_last_lr()[0]

                self.logger.info(
                    f"Epoch {epoch + 1:2d}/{self.config.training.epochs} | "
                    f"Loss: {avg_loss:.4f} | LR: {lr:.6f} | Time: {elapsed:.1f}s"
                )

                # ä¿å­˜æ£€æŸ¥ç‚¹
                if (epoch + 1) % self.config.checkpoint.save_interval == 0:
                    self.checkpoint_manager.save_checkpoint(epoch, self.model, self.optimizer, self.scaler, avg_loss)

        if self.rank == 0:
            total_time = time.time() - start_time
            self.logger.info(f"âœ… Training completed in {total_time:.1f}s on {self.world_size} GPUs.")


# ============================================================================
# ä¸»å‡½æ•°å’Œå…¥å£ç‚¹
# ============================================================================


def train_worker(rank: int, world_size: int, config: Config):
    """è®­ç»ƒå·¥ä½œè¿›ç¨‹"""
    distributed_manager = DistributedManager(config.distributed)

    try:
        # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
        distributed_manager.setup(rank, world_size)

        # åˆ›å»ºå¹¶è¿è¡Œè®­ç»ƒå™¨
        trainer = Trainer(config, rank, world_size)
        trainer.train()

    except Exception as e:
        logger = Logger(rank)
        logger.exception(f"An error occurred in rank {rank}: {e}")
        raise
    finally:
        # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
        distributed_manager.cleanup()


def main():
    """ä¸»å‡½æ•°"""
    # è·å–é…ç½®
    config = Config.from_args_and_env()

    # æ€§èƒ½ä¼˜åŒ–è®¾ç½®
    torch.backends.cudnn.benchmark = True

    # è®¡ç®—æ€»çš„è¿›ç¨‹æ•°
    distributed_manager = DistributedManager(config.distributed)
    world_size = distributed_manager.get_world_size()

    # è®¾ç½®ä¸´æ—¶æ—¥å¿—ï¼ˆä¸»è¿›ç¨‹ï¼‰
    logger = Logger(0)

    if world_size > 1:
        # å¤šGPUåˆ†å¸ƒå¼è®­ç»ƒ
        logger.info("ğŸ”§ Distributed Training Configuration:")
        logger.info(
            f"   ğŸŒ World Size: {world_size} | Nodes: {config.distributed.num_nodes} | GPUs per node: {config.distributed.gpus_per_node}"
        )
        logger.info(f"   ğŸ·ï¸  Current node rank: {config.distributed.node_rank}")
        logger.info(f"   ğŸŒ Master: {config.distributed.master_addr}:{config.distributed.master_port}")
        logger.info(f"   âš™ï¸  torch.compile: {'Enabled' if config.optimization.use_compile else 'Disabled'}")
        logger.info(f"   âš¡ AMP: {'Enabled' if config.optimization.use_amp else 'Disabled'}")

        # å¯åŠ¨å¤šè¿›ç¨‹è®­ç»ƒ
        mp.spawn(
            train_worker,
            args=(world_size, config),
            nprocs=config.distributed.gpus_per_node,
            join=True,
        )
    elif world_size == 1:
        # å•GPUè®­ç»ƒ
        logger.info("ğŸš€ Starting single-GPU training...")
        train_worker(0, 1, config)
    else:
        logger.error("âŒ No GPUs found. Exiting.")
        return


if __name__ == "__main__":
    main()
