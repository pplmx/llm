{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Framework Quick Start\n",
    "\n",
    "This notebook demonstrates the core capabilities of the LLM framework:\n",
    "\n",
    "1. **Building a Model** - Create a Decoder-only Transformer\n",
    "2. **Training** - Train on synthetic data\n",
    "3. **Inference** - Generate text\n",
    "4. **Advanced Features** - Gradient checkpointing, GQA, MoE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from llm.inference import generate, stream_generate\n",
    "from llm.models.decoder import DecoderModel\n",
    "from llm.tokenization.simple_tokenizer import SimpleCharacterTokenizer\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building a Model\n",
    "\n",
    "Let's create a simple Decoder-only Transformer model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple tokenizer from sample text\n",
    "sample_text = [\n",
    "    \"hello world\",\n",
    "    \"the quick brown fox jumps over the lazy dog\",\n",
    "    \"machine learning is amazing\",\n",
    "    \"transformers are powerful models\",\n",
    "]\n",
    "tokenizer = SimpleCharacterTokenizer(sample_text)\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "print(f\"Sample encoding: 'hello' -> {tokenizer.encode('hello')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = DecoderModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    max_seq_len=64,\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training\n",
    "\n",
    "Let's train the model on synthetic data for a few steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic training data\n",
    "batch_size = 8\n",
    "seq_len = 32\n",
    "\n",
    "input_ids = torch.randint(0, tokenizer.vocab_size, (batch_size, seq_len), device=device)\n",
    "labels = torch.randint(0, tokenizer.vocab_size, (batch_size, seq_len), device=device)\n",
    "\n",
    "print(f\"Input shape: {input_ids.shape}\")\n",
    "print(f\"Labels shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "model.train()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "losses = []\n",
    "for step in range(20):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    logits = model(input_ids)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(logits.view(-1, tokenizer.vocab_size), labels.view(-1))\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    if step % 5 == 0:\n",
    "        print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "print(f\"\\nTraining complete! Loss: {losses[0]:.4f} -> {losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference\n",
    "\n",
    "Now let's generate some text using our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-streaming generation\n",
    "model.eval()\n",
    "\n",
    "prompt = \"hello\"\n",
    "generated = generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=20,\n",
    "    temperature=0.8,\n",
    ")\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Generated: '{generated}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming generation\n",
    "print(f\"Streaming: '{prompt}'\", end=\"\")\n",
    "for token in stream_generate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    prompt=prompt,\n",
    "    max_new_tokens=20,\n",
    "    temperature=0.8,\n",
    "):\n",
    "    print(token, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advanced Features\n",
    "\n",
    "### 4.1 Gradient Checkpointing\n",
    "\n",
    "Reduce memory usage during training by recomputing activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with gradient checkpointing\n",
    "model_gc = DecoderModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=128,\n",
    "    num_layers=4,\n",
    "    num_heads=4,\n",
    "    max_seq_len=64,\n",
    "    gradient_checkpointing=True,  # Enable checkpointing\n",
    ").to(device)\n",
    "\n",
    "print(f\"Gradient checkpointing enabled: {model_gc.gradient_checkpointing}\")\n",
    "\n",
    "# Can also toggle dynamically\n",
    "model_gc.disable_gradient_checkpointing()\n",
    "print(f\"After disable: {model_gc.gradient_checkpointing}\")\n",
    "\n",
    "model_gc.enable_gradient_checkpointing()\n",
    "print(f\"After enable: {model_gc.gradient_checkpointing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Grouped Query Attention (GQA)\n",
    "\n",
    "Use fewer KV heads for better memory efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with GQA\n",
    "model_gqa = DecoderModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    num_heads=8,\n",
    "    num_kv_heads=2,  # Use 2 KV heads instead of 8\n",
    "    max_seq_len=64,\n",
    ").to(device)\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randint(0, tokenizer.vocab_size, (1, 10), device=device)\n",
    "output = model_gqa(test_input)\n",
    "print(f\"GQA model output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 SwiGLU Activation\n",
    "\n",
    "Use SwiGLU activation in the MLP layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with SwiGLU\n",
    "model_swiglu = DecoderModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    max_seq_len=64,\n",
    "    use_glu=True,  # Enable SwiGLU\n",
    ").to(device)\n",
    "\n",
    "output = model_swiglu(test_input)\n",
    "print(f\"SwiGLU model output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Mixture of Experts (MoE)\n",
    "\n",
    "Use sparse expert routing for larger model capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model with MoE\n",
    "model_moe = DecoderModel(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=128,\n",
    "    num_layers=2,\n",
    "    num_heads=4,\n",
    "    max_seq_len=64,\n",
    "    use_moe=True,\n",
    "    num_experts=4,\n",
    "    top_k=2,\n",
    ").to(device)\n",
    "\n",
    "output = model_moe(test_input)\n",
    "print(f\"MoE model output shape: {output.shape}\")\n",
    "\n",
    "# Compare parameter counts\n",
    "moe_params = sum(p.numel() for p in model_moe.parameters())\n",
    "base_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nBase model params: {base_params:,}\")\n",
    "print(f\"MoE model params: {moe_params:,}\")\n",
    "print(f\"Ratio: {moe_params / base_params:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using the E2E Pipeline\n",
    "\n",
    "Run a complete train -> evaluate -> inference workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.utils.e2e import E2EConfig, run_e2e_pipeline\n",
    "\n",
    "# Configure and run E2E pipeline\n",
    "config = E2EConfig(\n",
    "    hidden_size=64,\n",
    "    num_layers=2,\n",
    "    num_heads=2,\n",
    "    epochs=3,\n",
    "    num_samples=100,\n",
    "    prompt=\"hello\",\n",
    ")\n",
    "\n",
    "result = run_e2e_pipeline(config, device)\n",
    "\n",
    "print(f\"Training: {result.initial_loss:.4f} -> {result.final_loss:.4f}\")\n",
    "print(f\"Perplexity: {result.perplexity:.2f}\")\n",
    "print(f\"Generated: '{result.generated_text}'\")\n",
    "print(f\"All checks passed: {result.all_passed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Training on real data**: See `scripts/train_simple_decoder.py`\n",
    "- **Using HuggingFace tokenizers**: See `docs/usage.md`\n",
    "- **Serving with API**: Run `llm-serve` and use OpenAI SDK\n",
    "- **Read the docs**: `docs/tutorial-cpu-llm.md` for comprehensive guide"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
